{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjbOSZD-ZnyJ"
      },
      "source": [
        "## Transformers\n",
        "\n",
        "Wei Li\n",
        "\n",
        "An example of Machine Translation from English to German using the Multi30k Dataset using transformers.\n",
        "\n",
        "References:\n",
        "- Attention is all you need: https://arxiv.org/pdf/1706.03762.pdf\n",
        "- The Annotated Transformer: https://nlp.seas.harvard.edu/annotated-transformer/\n",
        "- https://nlp.seas.harvard.edu/annotated-transformer/\n",
        "\n",
        "This notebook is adapted from and expanded on the original notebook example in CMU 11-785 (Bhiksha Raj & Rita Singh).\n",
        "\n",
        "See also embedding, pad pack sequenes and language models basics.\n",
        "\n",
        "<img src=\"./images/transformer1.png\" width=\"400\" height=\"430\">\n",
        "\n",
        "Source: Figure 1 https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENTr-mX3aCf3"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEquNncU6yMU",
        "outputId": "aaca2608-a2d7-4b89-956c-5991d9ef7eac"
      },
      "outputs": [],
      "source": [
        "# !nvidia-smi\n",
        "\n",
        "# Uncomment to install\n",
        "# !pip install -U torchtext\n",
        "# !pip install -U pip setuptools wheel\n",
        "# !pip install spacy\n",
        "# !python -m spacy download \"de_core_news_sm\"\n",
        "# !python -m spacy download \"en_core_web_sm\"\n",
        "# !pip install portalocker>=2.0\n",
        "# !pip install -U torchdata\n",
        "# !pip install sacrebleu\n",
        "# !pip install torchsummaryX\n",
        "# You may need to restart your runtime after this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!python -m spacy download en_core_web_sm  # English language model.\n",
        "#!python -m spacy download de_core_news_sm #  German language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6b3f4trvaDY3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last updated: 2024-01-02 20:14:08\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.8.17\n",
            "IPython version      : 8.12.2\n",
            "\n",
            "numpy    : 1.21.5\n",
            "torch    : 1.12.1\n",
            "sacrebleu: 2.3.1\n",
            "torchtext: 0.13.1\n",
            "spacy    : 3.5.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sacrebleu\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from typing import Tuple\n",
        "import torchdata\n",
        "import spacy\n",
        "import random\n",
        "\n",
        "# %pip install watermark\n",
        "%load_ext watermark\n",
        "%watermark -u -t -d -v -p numpy,torch,sacrebleu,torchtext,spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils_evaluation import (\n",
        "    set_all_seeds,\n",
        "    set_deterministic,\n",
        "    generate_tgt_mask,\n",
        "    generate_src_mask,\n",
        "    calculate_bleu,\n",
        "    inference,\n",
        "    evaluate_test_set_bleu,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Setting\n",
        "\n",
        "# RANDOM_SEED = 2022\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# set_all_seeds(RANDOM_SEED)\n",
        "# set_deterministic()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some main components and features included in `en_core_web_sm`:\n",
        "\n",
        "- Tokenization: The model can split a given text into individual words or tokens.\n",
        "- Part-of-speech (POS) Tagging: It assigns grammatical labels to each token, such as noun, verb, adjective, etc.\n",
        "- Named Entity Recognition (NER): This component identifies and classifies named entities in the text, such as person names, organizations, locations, etc.\n",
        "- Dependency Parsing: It analyzes the grammatical structure of a sentence and represents it as a dependency tree, showing how different words relate to each other.\n",
        "- Word Vectors: The model provides word embeddings, which are numerical representations of words that capture semantic similarities between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4dFf7aDD5TpQ"
      },
      "outputs": [],
      "source": [
        "# Use this cell if you get a UTF Encoding Error\n",
        "# import locale\n",
        "# def getpreferredencoding(do_setlocale = True):\n",
        "#     return \"UTF-8\"\n",
        "# locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT9PsIg8CccO",
        "outputId": "4cb9ef2f-02e0-4de1-f684-8de390cf6b3a"
      },
      "outputs": [],
      "source": [
        "# use server http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt\n",
        "\n",
        "# !mkdir Multi30k\n",
        "# !mkdir Multi30k/train/\n",
        "# !mkdir Multi30k/val/\n",
        "# !mkdir Multi30k/test/\n",
        "\n",
        "# !wget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz\n",
        "# !wget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n",
        "# !wget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/mmt16_task1_test.tar.gz\n",
        "\n",
        "# # Extract archives to respective folders\n",
        "# !tar -xf training.tar.gz -C Multi30k/train/\n",
        "# !tar -xf validation.tar.gz -C Multi30k/val/\n",
        "# !tar -xf mmt16_task1_test.tar.gz -C Multi30k/test/\n",
        "\n",
        "# # training.tar.gz is extracted into Multi30k/train/\n",
        "# # validation.tar.gz is extracted into Multi30k/val/\n",
        "# # mmt16_task1_test.tar.gz is extracted into Multi30k/test/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# note: the server http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt is down\n",
        "# use the following instead\n",
        "\n",
        "# !mkdir Multi30k\n",
        "# !mkdir Multi30k/train/\n",
        "# !mkdir Multi30k/val/\n",
        "# !mkdir Multi30k/test/\n",
        "\n",
        "# !wget https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\n",
        "# !wget https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\n",
        "# !wget https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\n",
        "\n",
        "# # Extract archives to respective folders\n",
        "# !tar -xf training.tar.gz -C Multi30k/train/\n",
        "# !tar -xf validation.tar.gz -C Multi30k/val/\n",
        "# !tar -xf mmt16_task1_test.tar.gz -C Multi30k/test/\n",
        "\n",
        "# # training.tar.gz is extracted into Multi30k/train/\n",
        "# # validation.tar.gz is extracted into Multi30k/val/\n",
        "# # mmt16_task1_test.tar.gz is extracted into Multi30k/test/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29001/29001 [00:00<00:00, 37041.68it/s]\n",
            "100%|██████████| 29001/29001 [00:01<00:00, 19266.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Vocab Size English 10837\n",
            "\n",
            "Vocab Size German 19214\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "root = \"Multi30k/\"\n",
        "DATA_DIR = \"../data/\"+root\n",
        "\n",
        "\n",
        "# Initialize tokenizers for English and German using spaCy models\n",
        "# 'spacy' indicates the type of tokenizer (from spaCy)\n",
        "# 'en_core_web_sm' is the small English model, 'de_core_news_sm' is the small German model\n",
        "en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "#  it returns a spaCy tokenizer instance.\n",
        "de_tokenizer = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
        "\n",
        "\n",
        "# Define a function to tokenize English text\n",
        "# Uses the English tokenizer to split the text into tokens\n",
        "def tokenize_en(text):\n",
        "    doc = en_tokenizer(str(text))\n",
        "    return [token for token in doc]\n",
        "\n",
        "\n",
        "# Define a function to tokenize German text\n",
        "# Uses the German tokenizer to split the text into tokens\n",
        "def tokenize_de(text):\n",
        "    doc = de_tokenizer(str(text))\n",
        "    return [token for token in doc]\n",
        "\n",
        "\n",
        "# Define a class to create a vocabulary from text\n",
        "class VOCAB:\n",
        "    # Initializer for the VOCAB class\n",
        "    # tokenizer: a function for tokenizing text\n",
        "    # min_freq: minimum frequency for a word to be included in the vocabulary\n",
        "    # data: the text data to build the vocabulary from\n",
        "    # special_tokens: a list of special tokens (like <pad>, <sos>, etc.)\n",
        "\n",
        "    # Attributes:\n",
        "    #       .stoi: returns the string-index dictionary\n",
        "    #       .itos: returns the the list of vocubulary strings (index-string)\n",
        "\n",
        "    # Methods:\n",
        "    #       build_vocab: build vocabulary object using data (a list of strings)\n",
        "    #       __len__: returns the length of the vocabulary\n",
        "    #       __get_item__: retrieve the index corresponding to the token from the stoi dictionary.\n",
        "    #                      If the token is not found in the dictionary, it returns the index of a special token <unk> instead.\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        min_freq=2,\n",
        "        data=None,\n",
        "        special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"],\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.min_freq = min_freq\n",
        "        self.special_tokens = special_tokens\n",
        "        self.build_vocab(data)\n",
        "\n",
        "    # Method to build the vocabulary\n",
        "    def build_vocab(self, data):\n",
        "        counter = Counter()\n",
        "        # Iterate over the data, tokenize each text and update the counter\n",
        "        for text in tqdm(data):\n",
        "            tokens = self.tokenizer(text)\n",
        "            counter.update(tokens)\n",
        "\n",
        "        # Filter tokens that meet the minimum frequency threshold\n",
        "        tokens = [token for token, freq in counter.items() if freq >= self.min_freq]\n",
        "\n",
        "        # Add special tokens to the start of the tokens list\n",
        "        tokens = self.special_tokens + tokens\n",
        "\n",
        "        # Create string-to-index mapping\n",
        "        self.stoi = {token: index for index, token in enumerate(tokens)}\n",
        "        self.itos = tokens  # Also create index-to-string mapping\n",
        "\n",
        "    # Return the length of the vocabulary\n",
        "    def __len__(self):\n",
        "        return len(self.stoi)\n",
        "\n",
        "    # Retrieve an item index from the vocabulary; return index for '<unk>' for unknown tokens\n",
        "    def __getitem__(self, token):\n",
        "        return self.stoi.get(token, self.stoi[\"<unk>\"])\n",
        "        # This line attempts to retrieve the index corresponding to the token from the stoi dictionary.\n",
        "        # If the token is not found in the dictionary, it returns the index of a special token <unk> instead.\n",
        "\n",
        "\n",
        "# File paths for the English and German training data\n",
        "# en_file = \"Multi30k/train/train.en\"\n",
        "# de_file = \"Multi30k/train/train.de\"\n",
        "\n",
        "en_file = DATA_DIR+\"train/train.en\"\n",
        "de_file = DATA_DIR+\"train/train.de\"\n",
        "\n",
        "# Open and read the English training data file\n",
        "with open(en_file, \"r\", encoding=\"utf8\") as f:\n",
        "    train_data_en = [text.strip() for text in f.readlines()]\n",
        "\n",
        "# Open and read the German training data file\n",
        "with open(de_file, \"r\", encoding=\"utf8\") as f:\n",
        "    train_data_de = [text.strip() for text in f.readlines()]\n",
        "\n",
        "# Create vocabulary objects for English and German training data\n",
        "# Here, min_freq is set to 1, meaning all tokens are included\n",
        "EN_VOCAB = VOCAB(tokenize_en, min_freq=1, data=train_data_en)\n",
        "DE_VOCAB = VOCAB(tokenize_de, min_freq=1, data=train_data_de)\n",
        "\n",
        "# Print the sizes of the created English and German vocabularies\n",
        "print(\"\\nVocab Size English\", len(EN_VOCAB))\n",
        "print(\"\\nVocab Size German\", len(DE_VOCAB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a custom dataset class for translation tasks\n",
        "class TranslationDataset(Dataset):\n",
        "    # Constructor for the dataset class\n",
        "    def __init__(self, en_data, de_data, src_tokenizer, tgt_tokenizer, src_vocab, tgt_vocab):\n",
        "        # Store English and German data (eahc is a list of strings)\n",
        "        self.en_data = en_data  # English data\n",
        "        self.de_data = de_data  # German data\n",
        "\n",
        "        # Store source and target tokenizers\n",
        "        self.src_tokenizer = src_tokenizer  # Tokenizer for the source language (English)\n",
        "        self.tgt_tokenizer = tgt_tokenizer  # Tokenizer for the target language (German)\n",
        "\n",
        "        # Store source and target vocabularies\n",
        "        self.src_vocab = src_vocab  # Vocabulary for the source language\n",
        "        self.tgt_vocab = tgt_vocab  # Vocabulary for the target language\n",
        "\n",
        "    # Method to get a single item from the dataset\n",
        "    def __getitem__(self, index):\n",
        "        # get the index-th tuple of source text and target text (both in torch tensor of indices of tokens)\n",
        "        # <sos> and <eos> are added here\n",
        "        # \n",
        "        # Return:\n",
        "        #   a tuple of (src_tensor, tgt_tensor)\n",
        "        #       src_tensor is tensor shape [seq_length] consisting of indices for a source sentence (not padded)\n",
        "        #       tgt_tensor is tensor shape [seq_length] consisting of indices for a target sentence (not padded)\n",
        "\n",
        "        # Get the source and target texts for the given index\n",
        "        src_txt, tgt_txt = self.en_data[index], self.de_data[index]\n",
        "        # src_txt, tgt_txt is a string\n",
        "\n",
        "        # Tokenize the source and target texts and convert tokens to indices\n",
        "        src_tokens = [self.src_vocab[token] for token in self.src_tokenizer(src_txt)]\n",
        "        # scr_tokens: a list of indices for tokens\n",
        "        tgt_tokens = [self.tgt_vocab[token] for token in self.tgt_tokenizer(tgt_txt)]\n",
        "\n",
        "        # Add start-of-sequence (<sos>) index and end-of-sequence (<eos>) index \n",
        "        src_tokens = [self.src_vocab['<sos>']] + src_tokens + [self.src_vocab['<eos>']]\n",
        "        tgt_tokens = [self.tgt_vocab['<sos>']] + tgt_tokens + [self.tgt_vocab['<eos>']]\n",
        "\n",
        "        # Convert the token lists to PyTorch tensors\n",
        "        src_tensor = torch.LongTensor(src_tokens)  # Tensor for source sentence\n",
        "        tgt_tensor = torch.LongTensor(tgt_tokens)  # Tensor for target sentence\n",
        "\n",
        "        return src_tensor, tgt_tensor\n",
        "\n",
        "    # Method to get the size of the dataset\n",
        "    def __len__(self):\n",
        "        # Returns the lengths of the dataset (lengths of the list) \n",
        "\n",
        "        # Ensure source and target datasets are of the same length\n",
        "        assert len(self.en_data) == len(self.de_data)\n",
        "\n",
        "        return len(self.en_data)  # Return the length of the dataset\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        # batch: a list of tuples, where each tuple contains a pair of tensors\n",
        "        # (one from the source language, and one from the target language)\n",
        "        # padding <pad> is added.\n",
        "        #\n",
        "        # Returns:\n",
        "        #   a tuple of (src_tensors, tgt_tensors)\n",
        "        #   src_tensors is tensor shape [batch_size, max_seq_length] consisting of indices for source (padded)\n",
        "        #   tgt_tensors is tensor shape [batch_size, max_seq_length] consisting of indices for target (padded)\n",
        "\n",
        "        # Unzip the batch to separate source (src) and target (tgt) tensors\n",
        "        src_tensors, tgt_tensors = zip(*batch)\n",
        "\n",
        "        # Pad the sequences in the batch for source and target languages.\n",
        "        # This makes all sequences in the batch the same length by adding padding tokens.\n",
        "        # 'pad_sequence' is a utility function that handles the padding.\n",
        "        # 'padding_value' is the token used for padding (index of <pad> token in vocabulary).\n",
        "        # 'batch_first=True' makes sure that the batch size is the first dimension of the output tensor.\n",
        "        # After padding, src_tensors and tgt_tensors will be 2D tensors of shape [batch_size, max_seq_length],\n",
        "        # where 'batch_size' is the number of items in the batch and 'max_seq_length' is the length\n",
        "        # of the longest sequence in the batch.\n",
        "        src_tensors = torch.nn.utils.rnn.pad_sequence(src_tensors, padding_value=self.src_vocab['<pad>'], batch_first=True)\n",
        "        tgt_tensors = torch.nn.utils.rnn.pad_sequence(tgt_tensors, padding_value=self.tgt_vocab['<pad>'], batch_first=True)\n",
        "\n",
        "        # Return the padded source and target tensors.\n",
        "        # Now, each tensor in the batch has the same length, and they are suitable\n",
        "        # for batch processing in models (like RNNs, LSTMs, Transformers, etc.).\n",
        "        return src_tensors, tgt_tensors\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we shall see later, during training (teacher forcing), the source sequences will be input in the encoder, while the target sequences will be input in the decoder (as *tartget-input* sequences), and also target output (as *target-output* sequences) in the loss computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# en_file = \"Multi30k/train/train.en\"\n",
        "# de_file = \"Multi30k/train/train.de\"\n",
        "en_file = DATA_DIR+\"train/train.en\"\n",
        "de_file = DATA_DIR+\"train/train.de\"\n",
        "\n",
        "# Open the English text file and read its contents\n",
        "with open(en_file, \"r\", encoding=\"utf8\") as f:\n",
        "    train_data_en = [text.strip() for text in f.readlines()]\n",
        "\n",
        "# Open the German text file and read its contents\n",
        "with open(de_file, \"r\", encoding=\"utf8\") as f:\n",
        "    train_data_de = [text.strip() for text in f.readlines()]\n",
        "\n",
        "# en_file = \"Multi30k/val/val.en\"\n",
        "# de_file = \"Multi30k/val/val.de\"\n",
        "en_file = DATA_DIR+\"val/val.en\"\n",
        "de_file = DATA_DIR+\"val/val.de\"\n",
        "\n",
        "# Open the English text file and read its contents\n",
        "with open(en_file, \"r\", encoding=\"utf8\") as f:\n",
        "    val_data_en = [text.strip() for text in f.readlines()]\n",
        "\n",
        "# Open the German text file and read its contents\n",
        "with open(de_file, \"r\", encoding=\"utf8\") as f:\n",
        "    val_data_de = [text.strip() for text in f.readlines()]\n",
        "\n",
        "# en_file = \"Multi30k/test/test.en\"\n",
        "# de_file = \"Multi30k/test/test.de\"\n",
        "en_file = DATA_DIR+\"test/test.en\"\n",
        "de_file = DATA_DIR+\"test/test.de\"\n",
        "\n",
        "# Open the English text file and read its contents\n",
        "with open(en_file, \"r\", encoding=\"utf8\") as f:\n",
        "    test_data_en = [text.strip() for text in f.readlines()]\n",
        "\n",
        "# Open the German text file and read its contents\n",
        "with open(de_file, \"r\", encoding=\"utf8\") as f:\n",
        "    test_data_de = [text.strip() for text in f.readlines()]\n",
        "\n",
        "train_dataset = TranslationDataset(\n",
        "    train_data_en, train_data_de, tokenize_en, tokenize_de, EN_VOCAB, DE_VOCAB\n",
        ")\n",
        "val_dataset = TranslationDataset(\n",
        "    val_data_en, val_data_de, tokenize_en, tokenize_de, EN_VOCAB, DE_VOCAB\n",
        ")\n",
        "test_dataset = TranslationDataset(\n",
        "    test_data_en, test_data_de, tokenize_en, tokenize_de, EN_VOCAB, DE_VOCAB\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=train_dataset.collate_fn,\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=val_dataset.collate_fn\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=test_dataset.collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([ 1, 15, 16, 17, 18, 19,  9, 20, 21, 22, 23, 24, 14,  2]),\n",
              " tensor([ 1, 17,  7, 18, 19, 20, 21, 22, 16,  2]))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<sos> Two young , White males are outside near many bushes . <eos>',\n",
              " '<sos> Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche . <eos>')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the sentence\n",
        "' '.join([EN_VOCAB.itos[i] for i in train_dataset[0][0]]), ' '.join([DE_VOCAB.itos[i] for i in train_dataset[0][1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('A man in an orange hat starring at something.',\n",
              " 'Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# orginal sentences\n",
        "test_data_en[0], test_data_de[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<sos> A man in an orange hat starring at something . <eos>',\n",
              " '<sos> Ein Mann mit einem orangefarbenen Hut , der etwas <unk> . <eos>')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# what processed sentences look like through TranslationDataset class.\n",
        "' '.join([EN_VOCAB.itos[i] for i in test_dataset[0][0]]), ' '.join([DE_VOCAB.itos[i] for i in test_dataset[0][1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall that that each iteration from `train_dataloader` returns `(src, tgt)`, where src is `[batch_size, max_seq_length_in_source_in_the_batch]`,  and tgt shape `[batch_size, max_seq_length_in_target_in_the_batch]`.  We write a function to find out the max length in the source (across all batches), and the max length in the target (across all batches)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length in source across all batches: 43\n",
            "Max length in target across all batches: 46\n"
          ]
        }
      ],
      "source": [
        "def find_batch_max_lengths(train_dataloader):\n",
        "    max_lengths_src = []  # List to store max lengths of source sequences for each batch\n",
        "    max_lengths_tgt = []  # List to store max lengths of target sequences for each batch\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        # Append max lengths for this batch\n",
        "        max_lengths_src.append(src.shape[1])  # Second dimension is (padded) sequence length for source in the batch\n",
        "        max_lengths_tgt.append(tgt.shape[1])  # Second dimension is (padded) sequence length for target in the batch\n",
        "\n",
        "    return max_lengths_src, max_lengths_tgt\n",
        "\n",
        "# Example usage:\n",
        "max_lengths_src, max_lengths_tgt = find_batch_max_lengths(train_dataloader)\n",
        "\n",
        "# Finding the overall maximum length in source and target across all batches\n",
        "max_length_src = max(max_lengths_src)\n",
        "max_length_tgt = max(max_lengths_tgt)\n",
        "\n",
        "print(f\"Max length in source across all batches: {max_length_src}\")\n",
        "print(f\"Max length in target across all batches: {max_length_tgt}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkkZi2XkaHxO"
      },
      "source": [
        "## Attention mechanism\n",
        "\n",
        "<img src=\"./images/transformer_attentions.png\" width=\"400\" height=\"230\">\n",
        "\n",
        "Source: Figure 2 https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "Multi-Head Self-Attention is a key component of the Transformer architecture. It is designed to capture the dependencies between words in an input sequence without requiring sequential processing, which is the main drawback of RNNs and LSTMs.\n",
        "\n",
        "The idea behind self-attention is to compute a weighted sum of all words in the input sequence, where the weights are determined by how relevant each word in the entire sequence is to the current one under consideration. This mechanism allows the model to consider the entire context when processing each word.\n",
        "\n",
        "In Multi-Head Self-Attention, this process is performed multiple times (in parallel) with different linear projections of the input, which allows the model to capture different types of relationships between words (like Subject-Verb, Adjective-Adverb, Subject-Object, etc). These multiple attention \"heads\" are then concatenated and projected to create the final output of the self-attention layer.\n",
        "\n",
        "The computation of the self-attention weights involves three learnable vectors for each word: Query (Q), Key (K), and Value (V). The dot product between the query and the key determines the relevance score of each word, which is then normalized using the softmax function. Finally, the weighted sum of the value vectors produces the output for each word.\n",
        "\n",
        "(From the Paper)\n",
        "The Transformer uses multi-head attention in three different ways:\n",
        "\n",
        "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models.\n",
        "\n",
        "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
        "\n",
        "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
        "of the softmax which correspond to illegal connections.\n",
        "\n",
        "When we attend to sequences, attending to padding tokens in the input sequences is undesired for the Encoder, while attending to the future tokens in the target sequences is not desired in the decoder. To prevent this, we create source and target masks that can be used to nullify the influence of these tokens when calculating our context and attention weights. The **source mask** is used to prevent attention to the padding tokens in the source sequence, while the **target mask** ensures that the decoder only attends to the previous tokens in the target sequence during training (causal masking)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### masks\n",
        "\n",
        "In the following code, the attention score will be of shape `(batch_size, num_heads, seq_length_q, seq_length_k)` (3rd dim is the entry in queries, 4th dim is the entry in keys), the mask tensor should be compatible with these dimensions for the masking operation to work correctly. Here are the details:\n",
        "\n",
        "Purpose of Mask:\n",
        "\n",
        "The mask tensor is used to prevent the model from attending to certain positions within the sequence. This is typically used for two purposes:\n",
        "\n",
        "1. Padding Mask: To ignore padded positions in the input sequences, ensuring that the model does not treat padding tokens as meaningful input.\n",
        "2. Look-Ahead Mask: In sequence-to-sequence tasks like language translation, to prevent positions from attending to future positions in the sequence. This is particularly important during *training* to preserve the auto-regressive property.\n",
        "\n",
        "Shape of Mask:\n",
        "\n",
        "1. For Padding Mask, the mask is usually created based on the input sequences' lengths and has an initial shape of `(batch_size, seq_length_k)`. However, to align with the attention scores, it needs to be reshaped or broadcasted to `(batch_size, 1, 1, seq_length_k)`. This allows the same mask to be applied across all heads and all positions in the sequence.\n",
        "2. For Look-Ahead Mask, the shape is typically `(batch_size, 1, seq_length_q, seq_length_q)`, as it's the same for all sequences and heads. It's designed to mask future positions (lower triangle of the matrix) and is broadcastable to the shape of the attention scores.\n",
        "\n",
        "Broadcasting in Masking Operation:\n",
        "\n",
        "PyTorch (and other deep learning frameworks) often use broadcasting rules, allowing tensors of different shapes to be combined in a meaningful way. For example, a mask of shape `(batch_size, 1, 1, seq_length_k)` can be applied to an attention score tensor of shape `(batch_size, num_heads, seq_length_q, seq_length_k)` due to broadcasting.\n",
        "\n",
        "Application of Mask:\n",
        "\n",
        "The mask is applied in the attention score computation, usually by setting the scores of masked positions to a large negative value (like -inf) before the softmax operation. This ensures that these positions get an attention weight close to zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we compute the attention function on a set of queries\n",
        "simultaneously, packed together into a matrix $Q$.  The keys and\n",
        "values are also packed together into matrices $K$ and $V$.  We\n",
        "compute the matrix of outputs as:\n",
        "\n",
        "$$\n",
        "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SingleHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A version of single head attention.\n",
        "\n",
        "    Allowing dropout after calculating attention probability; and applying linear output layer\n",
        "    after attention values.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the SingleHeadAttention module.\n",
        "        d_model (int): The dimensionality of the input embedding and output embedding.\n",
        "                       i.e., we assume equal dimensionality for input and output        \n",
        "        \"\"\"\n",
        "        super(SingleHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Linear layers for transforming the queries, keys, and values.\n",
        "        # Each has input and output dimensions equal to d_model.\n",
        "        self.wq = nn.Linear(d_model, d_model)  # Query transformation\n",
        "        self.wk = nn.Linear(d_model, d_model)  # Key transformation\n",
        "        self.wv = nn.Linear(d_model, d_model)  # Value transformation\n",
        "\n",
        "        # Output linear transformation, again with input and output dimensions d_model.\n",
        "        self.wo = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Dropout layer for regularization.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for SingleHeadAttention.\n",
        "\n",
        "        Args:\n",
        "            q (Tensor): The input query tensor of shape (batch_size, seq_length_q, d_model).\n",
        "            k (Tensor): The input key tensor of shape (batch_size, seq_length_k, d_model). \n",
        "            v (Tensor): The input value tensor of shape (batch_size, seq_length_v, d_model).\n",
        "                        seq_length_k=seq_length_v: the seq lengths of the input\n",
        "                        if self-attention, then seq_length_k=seq_length_v=seq_length_k=seq_length_q\n",
        "            mask (Tensor, optional): The mask tensor for ignoring certain elements. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            a tuple of out, attn_scores, attn\n",
        "            out: The output of attended values--tensor of shape (batch_size, seq_length_q, d_model).\n",
        "            attn_scores: attention scores matrix (batch_size, seq_length_q, seq_length_k)\n",
        "            attn: attention probability matrix (batch_size, seq_length_q, seq_length_k)\n",
        "        \"\"\"\n",
        "\n",
        "        # Extract batch size from the query tensor.\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Linearly transform queries, keys, and values.\n",
        "        # Shapes after transformation: (batch_size, seq_length, d_model)\n",
        "        q = self.wq(q)  # Transformed queries\n",
        "        k = self.wk(k)  # Transformed keys\n",
        "        v = self.wv(v)  # Transformed values\n",
        "\n",
        "        # Calculate attention scores.\n",
        "        # Shape of attn_scores: (batch_size, seq_length_q, seq_length_k)\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_model ** 0.5)\n",
        "\n",
        "        # Apply the mask if provided.\n",
        "        # The mask shapes should be compatible with attn_scores shape.\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "        \n",
        "        # Apply softmax to the attention scores, followed by dropout.\n",
        "        # Shape of attn remains: (batch_size, seq_length_q, seq_length_k)\n",
        "        attn = self.dropout(F.softmax(attn_scores, dim=-1))\n",
        "\n",
        "        # Apply the attention to the values.\n",
        "        # Shape of out: (batch_size, seq_length_q, d_model)\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        # Pass the result through the final linear layer.\n",
        "        # Shape of out remains: (batch_size, seq_length_q, d_model)\n",
        "        out = self.wo(out)\n",
        "\n",
        "        return out, attn_scores, attn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 3, 4])\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([3, 3, 4]),\n",
              " tensor([[[0.3358, 0.3484, 0.3158, 0.0000],\n",
              "          [0.3383, 0.3420, 0.3197, 0.0000],\n",
              "          [0.3367, 0.3442, 0.3191, 0.0000]],\n",
              " \n",
              "         [[0.5059, 0.4941, 0.0000, 0.0000],\n",
              "          [0.4982, 0.5018, 0.0000, 0.0000],\n",
              "          [0.5012, 0.4988, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.2432, 0.2576, 0.2503, 0.2489],\n",
              "          [0.2443, 0.2489, 0.2554, 0.2513],\n",
              "          [0.2429, 0.2540, 0.2530, 0.2500]]], grad_fn=<SoftmaxBackward0>))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Source Mask Example\n",
        "# Create the single head attention model\n",
        "d_model = 2\n",
        "model = SingleHeadAttention(d_model, dropout=0.0)\n",
        "\n",
        "# Define sample query, key, and value tensors\n",
        "batch_size = 3\n",
        "seq_length_q = 3\n",
        "seq_length_k = 4\n",
        "seq_length_v = 4\n",
        "\n",
        "q = torch.rand(batch_size, seq_length_q, d_model)\n",
        "k = torch.rand(batch_size, seq_length_k, d_model)\n",
        "v = torch.rand(batch_size, seq_length_v, d_model)\n",
        "\n",
        "# Consider a source mask (assuming padding tokens at the end of the sequence)\n",
        "# Here, 1 indicates non-padding token, 0 indicates padding token\n",
        "source_mask = torch.tensor([\n",
        "    [1, 1, 1, 0],  # First sequence has one padding token at the end\n",
        "    [1, 1, 0, 0],  # Second sequence has two padding tokens at the end\n",
        "    [1, 1, 1, 1]   # Third sequence has no padding tokens\n",
        "])\n",
        "# print(source_mask.shape) # (3, 4) (batch_size, seq_length_k)\n",
        "\n",
        "# Reshape and expand the source mask to match the shape of attention scores\n",
        "source_mask = source_mask.unsqueeze(1).expand(batch_size, seq_length_q, seq_length_k)\n",
        "\n",
        "# Output the shape of the source mask\n",
        "print(source_mask.shape) # (3, 3, 4) (batch_size, seq_length_q, seq_length_k)\n",
        "\n",
        "# Forward pass through the model with the source mask\n",
        "output_source_masked = model(q, k, v, mask=source_mask)\n",
        "\n",
        "print()\n",
        "output_source_masked[2].shape, output_source_masked[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ True, False, False],\n",
            "         [ True,  True, False],\n",
            "         [ True,  True,  True]],\n",
            "\n",
            "        [[ True, False, False],\n",
            "         [ True,  True, False],\n",
            "         [ True,  True,  True]],\n",
            "\n",
            "        [[ True, False, False],\n",
            "         [ True,  True, False],\n",
            "         [ True,  True,  True]]])\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([3, 3, 3]),\n",
              " tensor([[[1.0000, 0.0000, 0.0000],\n",
              "          [0.4815, 0.5185, 0.0000],\n",
              "          [0.3263, 0.3553, 0.3184]],\n",
              " \n",
              "         [[1.0000, 0.0000, 0.0000],\n",
              "          [0.5005, 0.4995, 0.0000],\n",
              "          [0.3320, 0.3323, 0.3356]],\n",
              " \n",
              "         [[1.0000, 0.0000, 0.0000],\n",
              "          [0.5092, 0.4908, 0.0000],\n",
              "          [0.3448, 0.3307, 0.3245]]], grad_fn=<SoftmaxBackward0>))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Look-Ahead Mask Example\n",
        "# Create a Look-Ahead Mask to prevent positions from attending to future positions\n",
        "# look_ahead_mask: shape (batch_size, seq_length_q, seq_length_q)\n",
        "\n",
        "# Define sample query, key, and value tensors\n",
        "batch_size = 3\n",
        "seq_length_q = 3\n",
        "seq_length_k = 3  # Adjusted for self-attention (seq_length_k = seq_length_q)\n",
        "seq_length_v = 3  # Adjusted for self-attention (seq_length_v = seq_length_q)\n",
        "\n",
        "q = torch.rand(batch_size, seq_length_q, d_model)\n",
        "k = torch.rand(batch_size, seq_length_k, d_model)\n",
        "v = torch.rand(batch_size, seq_length_v, d_model)\n",
        "\n",
        "# Create a Look-Ahead Mask for self-attention\n",
        "# This mask will be lower triangular (including diagonal), allowing each position\n",
        "# to attend to itself and past positions, but not future ones\n",
        "look_ahead_mask = torch.tril(torch.ones((batch_size, seq_length_q, seq_length_q))).bool()\n",
        "\n",
        "print(look_ahead_mask)\n",
        "\n",
        "# Forward pass through the model with the look-ahead mask\n",
        "output_look_ahead_masked = model(q, k, v, mask=look_ahead_mask)\n",
        "\n",
        "print()\n",
        "output_look_ahead_masked[2].shape, output_look_ahead_masked[2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### multi-head attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "F1YFaLtvX5sA"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        \"\"\"\n",
        "        Initialize the MultiHeadSelfAttention module.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the input embedding and output embedding.\n",
        "                        i.e., we assume equal dimensionality for input and output for the attention layer\n",
        "                            In the self.forward method, \n",
        "                            q (Tensor): The input query tensor of shape (batch_size, seq_length_q, d_model).\n",
        "                            k (Tensor): The input key tensor of shape (batch_size, seq_length_k, d_model). \n",
        "                            v (Tensor): The input value tensor of shape (batch_size, seq_length_v, d_model).\n",
        "            num_heads (int): The number of attention heads.\n",
        "\n",
        "        Description:\n",
        "            This module implements the multi-head self-attention mechanism as described in the\n",
        "            \"Attention Is All You Need\" paper. It splits the input into multiple heads, allowing the\n",
        "            model to jointly attend to information from different representation subspaces at different\n",
        "            positions. Each head processes a portion of the input independently, and their outputs\n",
        "            are concatenated and linearly transformed into the expected dimensionality.        \n",
        "\n",
        "            So for each head, dim_k=dim_v=d_model/num_heads,\n",
        "            When num_heads=1, dim_k=dim_v=d_model.\n",
        "\n",
        "            ref. Vaswani Fig 2.\n",
        "        \"\"\"\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads # each head handles a porportion to main the same d_model\n",
        "\n",
        "        assert self.head_dim * num_heads == d_model, \"Invalid number of heads or d_model dimensions\"\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model) # in_features, out_features = d_model, d_model\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.wo = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for MultiHeadSelfAttention.\n",
        "        We assume equal dimensionality for input and output for the attention layer=d_model\n",
        "\n",
        "        Args:\n",
        "            q (Tensor): The input query tensor of shape (batch_size, seq_length_q, d_model).\n",
        "            k (Tensor): The input key tensor of shape (batch_size, seq_length_k, d_model). \n",
        "            v (Tensor): The input value tensor of shape (batch_size, seq_length_v, d_model).\n",
        "                        seq_length_k=seq_length_v: the seq lengths of the input\n",
        "                        If self-attention, then seq_length_k=seq_length_v=seq_length_k=seq_length_q\n",
        "            mask (Tensor, optional): The mask tensor for ignoring certain elements. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output of attended values--tensor of shape (batch_size, seq_length_q, d_model).\n",
        "        \"\"\"\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "        # self.wq(q): [batch_size, seq_length_q, d_model].\n",
        "        # -1 is a placeholder that tells PyTorch to automatically calculate the \n",
        "        #   appropriate size for this dimension, which in this context will be seq_length.\n",
        "        # self.num_heads is the number of attention heads. This dimension is explicitly set, \n",
        "        #   splitting the d_model dimension into num_heads smaller heads.\n",
        "        # self.head_dim is the dimensionality of each head, calculated as d_model // num_heads. \n",
        "        #   It indicates how many features each head will process.\n",
        "        # q shape (batch_size, seq_length_q, num_heads, head_dim)\n",
        "\n",
        "        # Each head can focus on different features of the input\n",
        "\n",
        "        # Transpose to get dimensions (batch_size, num_heads, seq_length, head_dim)\n",
        "        # Now, each head's data is grouped together\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # do computation head-wise:\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        # q has a shape of (batch_size, num_heads, seq_length_q, head_dim).\n",
        "        # k.transpose(-2, -1) has a shape of (batch_size, num_heads, head_dim, seq_length_k).\n",
        "        # The matrix multiplication between q and the transposed k results in a tensor of shape \n",
        "        # (batch_size, num_heads, seq_length_q, seq_length_k). \n",
        "        # This shape represents attention scores for each query against all keys in all heads for each sequence in the batch.        \n",
        "\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Softmax to compute attention weights\n",
        "        attn = F.softmax(attn_scores, dim=-1)\n",
        "        # attn: (batch_size, num_heads, seq_length_q, seq_length_k)\n",
        "\n",
        "        out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        # v: (batch_size, num_heads, seq_length_v, head_dim)   \n",
        "        # seq_length_k=seq_length_v\n",
        "        # torch.matmul(attn, v): (batch_size, num_heads, seq_length_q, head_dim)\n",
        "        # .transpose(1, 2) changes the shape to (batch_size, seq_length_q, num_heads, head_dim).\n",
        "        # .contiguous() is used as a safety measure to ensure that the tensor is \n",
        "        # stored in a contiguous block of memory, which is required for the subsequent .view() operation.\n",
        "        # .view(batch_size, -1, self.d_model) reshapes the tensor back to (batch_size, seq_length_q, d_model).\n",
        "        # note that d_model = num_heads*head_dim\n",
        "    \n",
        "        out = self.wo(out) # (batch_size, seq_length_q, d_model)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld6U3RBovfIZ"
      },
      "source": [
        "## Position-wise Feed Forward Networks\n",
        "\n",
        "An FFN consists of two linear layers with a non-linear activation function in between, such as ReLU (Rectified Linear Unit). This (two-layer) feed forward network is applied position-wise (i.e., applied to *each position* separately and identically). The output of the first linear layer increases the dimensionality of the input, while the second linear layer reduces it back to the original dimension. This expansion and reduction allow the FFN to learn complex patterns and relationships between features.\n",
        "\n",
        "The FFN is applied after the Multi-Head Self-Attention layer in both the encoder and decoder blocks of the Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n",
        "\n",
        "While the linear transformations are the same across different\n",
        "positions, they use different parameters from layer to\n",
        "layer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CsJXFzQevYyU"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the PositionwiseFeedForward module. \n",
        "        Note that dropout is applied here after the first layer instead of the lasy layer.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the input embedding and output embedding.\n",
        "            d_ff (int): The dimensionality of the hidden layer in the feed-forward network (generally larger than d_model).\n",
        "            dropout (float, optional): The dropout probability. Defaults to 0.1.\n",
        "        \"\"\"\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for PositionwiseFeedForward.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        out = self.linear1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.linear2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J19HpBEwQgq"
      },
      "source": [
        "## Add and Norm\n",
        "\n",
        "The Add & Norm module is crucial for maintaining a stable gradient flow in deep networks, such as the Transformer. It consists of two parts: residual connections and layer normalization.\n",
        "\n",
        "The Add & Norm module is applied after both the Multi-Head Self-Attention layer and the Position-wise Feed-Forward layer in the Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1dlNCAhJwNBY"
      },
      "outputs": [],
      "source": [
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        \"\"\"\n",
        "        Initialize the AddNorm module.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the input and output.\n",
        "            eps (float, optional): A small constant for numerical stability. Defaults to 1e-6.\n",
        "        \"\"\"\n",
        "        super(AddNorm, self).__init__()\n",
        "        self.norm = nn.LayerNorm(d_model, eps=eps)\n",
        "        # d_model is the dimensionality of the input and output tensors, and \n",
        "        # it is also the normalization dimension for LayerNorm. This means the\n",
        "        # normalization is applied across the d_model features of the input tensor.\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        \"\"\"\n",
        "        Forward pass for AddNorm.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
        "            residual (Tensor): The residual tensor of the same shape as the input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        out = x + residual\n",
        "        out = self.norm(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH7SlGJ5wa1r"
      },
      "source": [
        "  ## Positional Encoding\n",
        "\n",
        "Positional Encoding is a technique used in the Transformer architecture to inject positional information into the input sequence. Since the Multi-Head Self-Attention mechanism is **permutation-equivariant**, it cannot capture the relative position of words in the input sequence by itself. Positional Encoding helps to address this issue by adding a fixed vector to each word's embeddings, which is computed based on its position in the sequence.\n",
        "\n",
        "The positional encoding function used in the Transformer is based on sine and cosine functions with different frequencies. This choice allows the model to learn to attend to both nearby and distant words, as well as to interpolate the positional encoding for longer sequences than those seen during training.\n",
        "\n",
        "Note for $i=0, \\ldots, d_{model}/2$, the wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot2\\pi$.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$PE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
        "\n",
        "$$PE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})$$\n",
        "\n",
        "Note: $1/10000^{2i/d_{\\text{model}}}=\\exp(-2i\\log(1000)/d_{\\text{model}} )$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "B24BF-sRwZQq"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the PositionalEncoding module.\n",
        "        We apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. \n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the input.\n",
        "            max_seq_len (int): The maximum length of the input sequence.\n",
        "            dropout (float, optional): The dropout probability. Defaults to 0.1.\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "\n",
        "        # Create a tensor representing position indices from 0 to max_seq_len\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Compute the division term used in the formulas for sine and cosine\n",
        "        # The division term ensures different wavelengths for different dimensions\n",
        "        # In torch.arange(0, d_model, 2), it starts at 0, ends before d_model, and increments by 2.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "\n",
        "        # Compute sine values for even indices in the positional encoding matrix\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Compute cosine values for odd indices in the positional encoding matrix\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add an extra dimension to the positional encoding for batch size\n",
        "        pe = pe.unsqueeze(0)\n",
        "        # pe (batch_size, max_seq_len, d_model)\n",
        "\n",
        "        # Register 'pe' as a buffer that is not a model parameter.\n",
        "        # Buffers, such as running averages, are not updated by backpropagation.\n",
        "        # They are, however, saved and restored in the state_dict and moved to\n",
        "        # GPU along with the model during .to(device) calls.\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for PositionalEncoding.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        # pe added to x should have same seq_length as x\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfOgQ_-KzU83"
      },
      "source": [
        "## Encoder Block\n",
        "\n",
        "The Encoder Block in the Transformer architecture consists of the following layers:\n",
        "\n",
        "- Multi-Head Self-Attention layer\n",
        "- Add & Norm (Residual connection and Layer Normalization)\n",
        "- Position-wise Feed-Forward Network layer\n",
        "- Add & Norm (Residual connection and Layer Normalization)\n",
        "\n",
        "In the Transformer, multiple encoder blocks (6 according to the paper) are stacked on top of each other to form the complete encoder module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "B8L4yET0wfPo"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the EncoderBlock module.\n",
        "        (multi_head_attend + add_norm + feed_forward + add_norm ) X 6\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the input (and output).\n",
        "            num_heads (int): The number of attention heads.\n",
        "            d_ff (int): The dimensionality of the hidden layer in the feed-forward network (generally larger than d_model)\n",
        "            dropout (float, optional): The dropout probability (used in positional encoding). Defaults to 0.1.\n",
        "        \"\"\"\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.norm1 = AddNorm(d_model)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.norm2 = AddNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for EncoderBlock.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
        "            mask (Tensor, optional): The mask tensor for ignoring certain elements. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        x1 = self.self_attn(x, x, x, mask)  # q=k=v=x (self attention)\n",
        "        x = self.norm1(x, x1)\n",
        "        x1 = self.ffn(x)\n",
        "        x = self.norm2(x, x1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF_x0G-qzilI"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "The Decoder Block in the Transformer architecture consists of the following layers:\n",
        "\n",
        "- Masked Multi-Head Self-Attention layer (optionally followed by dropout)\n",
        "- Add & Norm (Residual connection and Layer Normalization)\n",
        "- Encoder-Decoder Multi-Head Cross Attention layer (optionally followed by dropout)\n",
        "- Add & Norm (Residual connection and Layer Normalization)\n",
        "- Position-wise Feed-Forward Network layer (optionally followed by dropout)\n",
        "- Add & Norm (Residual connection and Layer Normalization) \n",
        "\n",
        "In the Transformer, multiple decoder blocks (6 per the paper) are stacked on top of each other to form the complete decoder module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kRyZNQuTzeNo"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the DecoderBlock module.\n",
        "        (multi_head_attend + add_norm + encoder_decoder_attend +  add_norm + feed_forward + add_norm ) X 6\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The total dimensionality of the input (and output).\n",
        "            num_heads (int): The number of attention heads.\n",
        "            d_ff (int): The dimensionality of the hidden layer in the feed-forward network.\n",
        "            dropout (float, optional): The dropout probability (used in PositionwiseFeedForward). Defaults to 0.1.\n",
        "        \"\"\"\n",
        "        \n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.norm1 = AddNorm(d_model)\n",
        "        self.enc_dec_attn = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.norm2 = AddNorm(d_model)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.norm3 = AddNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for DecoderBlock (as used in training).\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The \"target input\" tensor of shape (batch_size, x_seq_length, d_model). \n",
        "            enc_output (Tensor): The encoder output tensor of shape (batch_size, enc_output_seq_length, d_model). \n",
        "            src_mask (Tensor, optional): The source mask (padding mask on encoder output) tensor for ignoring certain elements. Defaults to None.\n",
        "            tgt_mask (Tensor, optional): The target mask (look-ahead mask + padding mask on target_input) tensor for ignoring certain elements. Defaults to None.\n",
        "\n",
        "            Each sequence in x is usually a \"target input\" sequence for training (teacher forcing).\n",
        "            Each sequence in enc_output is a sequence of length of that of a source sequence, which \n",
        "            is returned as output from self-attention layer in encoder.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor of shape (batch_size, x_seq_length, d_model).\n",
        "        \"\"\"\n",
        "        x1 = self.self_attn(x, x, x, tgt_mask)  # q=k=v=x (self attention)\n",
        "        x = self.norm1(x, x1)\n",
        "        x1 = self.enc_dec_attn(x, enc_output, enc_output, src_mask) # q=x, k=v=enc_output (cross attention)\n",
        "        x = self.norm2(x, x1)\n",
        "        x1 = self.ffn(x)\n",
        "        x = self.norm3(x, x1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-0cwvDg1OJY"
      },
      "source": [
        "## The Transformer\n",
        "\n",
        "Now that we have implemented all the building blocks, let's assemble the complete Transformer architecture.\n",
        "\n",
        "We initialize the following components:\n",
        "\n",
        "- Source and target embedding layers\n",
        "- Positional encoding module\n",
        "- Encoder and decoder layer stacks\n",
        "- Final linear layer to produce the probability distribution over the target vocabulary\n",
        "\n",
        "In the forward method, we first pass the source and target input tensors through their respective embedding layers and add the positional encoding. Then, we pass the source input through each encoder layer sequentially, followed by passing the target input and encoder output through each decoder layer sequentially. Finally, we apply the linear layer to produce the output tensor with shape (batch_size, tgt_seq_length, tgt_vocab_size)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0LaSAWRQ0Jo1"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, D_MODEL, num_heads, d_ff, max_seq_len, num_layers, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the Transformer module.\n",
        "\n",
        "        This module applies dropout to position-wise feed forward and in the positional encoding.\n",
        "\n",
        "        Args:\n",
        "            src_vocab_size (int): The size of the source vocabulary.\n",
        "            tgt_vocab_size (int): The size of the target vocabulary.\n",
        "            d_model (int): The dimensionality of the embedding\n",
        "            num_heads (int): The number of attention heads.\n",
        "            d_ff (int): The dimensionality of the hidden layer in the feed-forward network.\n",
        "            max_seq_len (int): The maximum length of the input sequence (as needed in PositionalEncoding)\n",
        "            num_layers (int): The number of layers in the encoder and decoder.\n",
        "            dropout (float, optional): The dropout probability. Defaults to 0.1.\n",
        "        \"\"\"\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # # Converts token indices to embeddings of dimension D_MODEL\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, D_MODEL)\n",
        "            # nn.Embedding args (num_embeddings, embedding_dim) = (src_vocab_size, D_MODEL)\n",
        "            # where num_embeddings =  size of the dictionary of embeddings\n",
        "            #       embedding_dim = the size of each embedding vector\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, D_MODEL)\n",
        "        self.pos_encoding = PositionalEncoding(D_MODEL, max_seq_len, dropout)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderBlock(D_MODEL, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderBlock(D_MODEL, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(D_MODEL, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for Transformer.\n",
        "\n",
        "        Args:\n",
        "        src (Tensor): The source input tensor of shape (batch_size, src_seq_length).\n",
        "        tgt (Tensor): The target input tensor of shape (batch_size, tgt_seq_length).\n",
        "        src_mask (Tensor, optional): The source mask (padding mask on encoder output) tensor for ignoring certain elements. Defaults to None.\n",
        "        tgt_mask (Tensor, optional): The target mask (look-ahead mask + padding mask on target input) tensor for ignoring certain elements. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "        Tensor: The output tensor of logits, shape (batch_size, tgt_seq_length, tgt_vocab_size).\n",
        "        \"\"\"\n",
        "        src = self.src_embedding(src)\n",
        "        src = self.pos_encoding(src)\n",
        "        # src after embedding and pos_encoding: (batch_size, src_seq_length, D_MODEL)\n",
        "\n",
        "        tgt = self.tgt_embedding(tgt)\n",
        "        tgt = self.pos_encoding(tgt)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        for layer in self.decoder_layers:\n",
        "            tgt = layer(tgt, src, src_mask, tgt_mask)\n",
        "\n",
        "        out = self.fc(tgt)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvQR5yVUVbrT"
      },
      "source": [
        "## Define Model and associated Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lrCiSmTeH0Bp"
      },
      "outputs": [],
      "source": [
        "# Define Hyper Parameters\n",
        "NUM_EPOCHS      = 5\n",
        "D_MODEL         = 256\n",
        "ATTN_HEADS      = 8\n",
        "NUM_LAYERS      = 3\n",
        "FEEDFORWARD_DIM = 512\n",
        "DROPOUT         = 0.1\n",
        "MAX_SEQ_LEN     = 150 \n",
        "SRC_VOCAB_SIZE  = len(EN_VOCAB)\n",
        "TGT_VOCAB_SIZE  = len(DE_VOCAB)\n",
        "LR              = 0\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8zuUegclXTcD"
      },
      "outputs": [],
      "source": [
        "class NoamScheduler:\n",
        "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
        "        \"\"\"\n",
        "        Initializes the NoamScheduler. A customized learning rate scheduler.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Optimizer whose learning rate will be scheduled.\n",
        "            d_model (int): Dimensionality of the model embeddings.\n",
        "            warmup_steps (int): Number of warmup steps for learning rate scheduling.\n",
        "        \"\"\"\n",
        "        self.optimizer = optimizer  # The optimizer to adjust the learning rate for\n",
        "        self.d_model = d_model  # Dimensionality of model embeddings\n",
        "        self.warmup_steps = warmup_steps  # Number of warmup steps\n",
        "        self.current_step = 0  # Counter for tracking the number of optimization steps\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Updates the learning rate for each parameter group in the optimizer.\n",
        "\n",
        "        This is called inside a trainining process.\n",
        "        \"\"\"\n",
        "        self.current_step += 1  # Increment the number of steps\n",
        "        lr = self.learning_rate()  # Calculate the new learning rate\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr  # Update the learning rate in the optimizer\n",
        "\n",
        "    def learning_rate(self):\n",
        "        \"\"\"\n",
        "        Computes the learning rate based on the current step.\n",
        "\n",
        "        Returns:\n",
        "            float: The computed learning rate.\n",
        "        \"\"\"\n",
        "        step = self.current_step\n",
        "        # The learning rate formula\n",
        "        return (self.d_model ** -0.5) * min(step ** -0.5, step * self.warmup_steps ** -1.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The learning rate formula used in the `NoamScheduler` can be represented as follows:\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{Learning Rate}=d_{\\text{model}}^{-0.5} \\cdot \\min \\left(\\operatorname{step}^{-0.5}, \\text{step} \\cdot \\text{warmup\\_steps}^{-1.5}\\right)\n",
        "\\end{align*}\n",
        "\n",
        "- During the initial `warmup_steps`, the learning rate increases linearly.\n",
        "- After `warmup_steps`, the learning rate decreases proportionally to the inverse square root of the step number.\n",
        "- The `d_model` term is used for scaling the learning rate according to the model size.\n",
        "\n",
        "This schedule allows for a rapid increase in the learning rate during the initial warmup phase, which is then moderated for a smoother descent, potentially leading to better training stability and performance for Transformer models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ChzftDC3ISN9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wli169/miniconda3/envs/py38torch_arm/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "model = Transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, D_MODEL, ATTN_HEADS, FEEDFORWARD_DIM, MAX_SEQ_LEN, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
        "# optimizer = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9, weight_decay=5e-2)\n",
        "\n",
        "warmup_steps = 2 * len(train_dataloader) # len(train_dataloader) =227 batches; warmup_steps = 2 epochs = total of 2*227 updates\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
        "# scheduler = LambdaLR(optimizer, lr_lambda=lambda step: (D_MODEL ** -0.5) * min((step + 1) ** -0.5, (step + 1) * warmup_steps ** -1.5), verbose=True)\n",
        "scheduler = NoamScheduler(optimizer, d_model=D_MODEL, warmup_steps=warmup_steps)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=DE_VOCAB['<pad>'], label_smoothing=0.1)\n",
        "# The function expects the model output to be raw, unnormalized scores (often called logits) for each class.\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On `torch.nn.CrossEntropyLoss(ignore_index=DE_VOCAB['<pad>'], label_smoothing=0.1)`:\n",
        "\n",
        "`ignore_index=DE_VOCAB['<pad>']`:\n",
        "\n",
        "`ignore_index` is a parameter that specifies a target value that is ignored in the loss computation.\n",
        "In this case, `DE_VOCAB['<pad>']` is the index of the padding token (<pad>) in the target (German) vocabulary.\n",
        "By setting ignore_index to the index of the padding token, the loss calculation will exclude these tokens. This is important in tasks like language translation where different sentences have different lengths, and padding is used to make all sentences in a batch have the same length.\n",
        "\n",
        "`label_smoothing=0.1`:\n",
        "\n",
        "`label_smoothing` is a technique used to make the model less confident in its predictions, by smoothing the hard labels in the target.\n",
        "The parameter 0.1 indicates the smoothing level. In this case, it means that a small fraction of the loss will be distributed across all classes, making the model's output distribution slightly softer.\n",
        "This can lead to better generalization and prevent the model from becoming too confident about its predictions, which can be beneficial in preventing overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSHCF8Roet4a"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([3, 1, 1, 4]),\n",
              " tensor([[[[ True,  True,  True, False]]],\n",
              " \n",
              " \n",
              "         [[[ True,  True, False, False]]],\n",
              " \n",
              " \n",
              "         [[[ True,  True,  True,  True]]]]))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example: illustrate generate_tgt_mask() and generate_src_mask()\n",
        "\n",
        "# Shape: (batch_size, max_seq_length_in_the_batch)= (3, 4)\n",
        "src = torch.tensor(\n",
        "    [\n",
        "        [1, 2, 3, 0],  # First sequence with a padding token\n",
        "        [4, 5, 0, 0],  # Second sequence with two padding tokens\n",
        "        [6, 7, 8, 9],  # Third sequence with no padding tokens\n",
        "    ]\n",
        ")\n",
        "\n",
        "pad_idx = 0  # Define the padding index (0 is the padded value)\n",
        "\n",
        "# Generate the source mask (batch_size, 1, 1, max_seq_length)\n",
        "src_mask = generate_src_mask(src, pad_idx)\n",
        "\n",
        "src_mask.shape, src_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([3, 1, 4, 4]),\n",
              " tensor([[[[ True, False, False, False],\n",
              "           [ True,  True, False, False],\n",
              "           [ True,  True,  True, False],\n",
              "           [ True,  True,  True, False]]],\n",
              " \n",
              " \n",
              "         [[[ True, False, False, False],\n",
              "           [ True,  True, False, False],\n",
              "           [ True,  True, False, False],\n",
              "           [ True,  True, False, False]]],\n",
              " \n",
              " \n",
              "         [[[ True, False, False, False],\n",
              "           [ True,  True, False, False],\n",
              "           [ True,  True,  True, False],\n",
              "           [ True,  True,  True,  True]]]]))"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example\n",
        "# Define a sample target input tensor with padding tokens (assuming pad_idx=0)\n",
        "# Shape: (batch_size, max_seq_length)\n",
        "tgt = torch.tensor([\n",
        "    [1, 2, 3, 0],  # First sequence with a padding token\n",
        "    [4, 5, 0, 0],  # Second sequence with two padding tokens\n",
        "    [6, 7, 8, 9]   # Third sequence with no padding tokens\n",
        "])\n",
        "\n",
        "pad_idx = 0  # Define the padding index\n",
        "\n",
        "# Generate the target mask (batch_size, 1, max_seq_length, max_seq_length)\n",
        "tgt_mask = generate_tgt_mask(tgt, pad_idx)\n",
        "\n",
        "tgt_mask.shape, tgt_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jXgt0QI7J7f"
      },
      "source": [
        "## Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Sj8iRQaa7Knt",
        "outputId": "cf0d100b-9e85-461b-bb9c-5dec88abb4a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================================\n",
            "                                            Kernel Shape      Output Shape  \\\n",
            "Layer                                                                        \n",
            "0_src_embedding                             [256, 10837]    [128, 26, 256]   \n",
            "1_pos_encoding.Dropout_dropout                         -    [128, 26, 256]   \n",
            "2_tgt_embedding                             [256, 19214]    [128, 29, 256]   \n",
            "3_pos_encoding.Dropout_dropout                         -    [128, 29, 256]   \n",
            "4_encoder_layers.0.self_attn.Linear_wq        [256, 256]    [128, 26, 256]   \n",
            "5_encoder_layers.0.self_attn.Linear_wk        [256, 256]    [128, 26, 256]   \n",
            "6_encoder_layers.0.self_attn.Linear_wv        [256, 256]    [128, 26, 256]   \n",
            "7_encoder_layers.0.self_attn.Linear_wo        [256, 256]    [128, 26, 256]   \n",
            "8_encoder_layers.0.norm1.LayerNorm_norm            [256]    [128, 26, 256]   \n",
            "9_encoder_layers.0.ffn.Linear_linear1         [256, 512]    [128, 26, 512]   \n",
            "10_encoder_layers.0.ffn.Dropout_dropout                -    [128, 26, 512]   \n",
            "11_encoder_layers.0.ffn.Linear_linear2        [512, 256]    [128, 26, 256]   \n",
            "12_encoder_layers.0.norm2.LayerNorm_norm           [256]    [128, 26, 256]   \n",
            "13_encoder_layers.1.self_attn.Linear_wq       [256, 256]    [128, 26, 256]   \n",
            "14_encoder_layers.1.self_attn.Linear_wk       [256, 256]    [128, 26, 256]   \n",
            "15_encoder_layers.1.self_attn.Linear_wv       [256, 256]    [128, 26, 256]   \n",
            "16_encoder_layers.1.self_attn.Linear_wo       [256, 256]    [128, 26, 256]   \n",
            "17_encoder_layers.1.norm1.LayerNorm_norm           [256]    [128, 26, 256]   \n",
            "18_encoder_layers.1.ffn.Linear_linear1        [256, 512]    [128, 26, 512]   \n",
            "19_encoder_layers.1.ffn.Dropout_dropout                -    [128, 26, 512]   \n",
            "20_encoder_layers.1.ffn.Linear_linear2        [512, 256]    [128, 26, 256]   \n",
            "21_encoder_layers.1.norm2.LayerNorm_norm           [256]    [128, 26, 256]   \n",
            "22_encoder_layers.2.self_attn.Linear_wq       [256, 256]    [128, 26, 256]   \n",
            "23_encoder_layers.2.self_attn.Linear_wk       [256, 256]    [128, 26, 256]   \n",
            "24_encoder_layers.2.self_attn.Linear_wv       [256, 256]    [128, 26, 256]   \n",
            "25_encoder_layers.2.self_attn.Linear_wo       [256, 256]    [128, 26, 256]   \n",
            "26_encoder_layers.2.norm1.LayerNorm_norm           [256]    [128, 26, 256]   \n",
            "27_encoder_layers.2.ffn.Linear_linear1        [256, 512]    [128, 26, 512]   \n",
            "28_encoder_layers.2.ffn.Dropout_dropout                -    [128, 26, 512]   \n",
            "29_encoder_layers.2.ffn.Linear_linear2        [512, 256]    [128, 26, 256]   \n",
            "30_encoder_layers.2.norm2.LayerNorm_norm           [256]    [128, 26, 256]   \n",
            "31_decoder_layers.0.self_attn.Linear_wq       [256, 256]    [128, 29, 256]   \n",
            "32_decoder_layers.0.self_attn.Linear_wk       [256, 256]    [128, 29, 256]   \n",
            "33_decoder_layers.0.self_attn.Linear_wv       [256, 256]    [128, 29, 256]   \n",
            "34_decoder_layers.0.self_attn.Linear_wo       [256, 256]    [128, 29, 256]   \n",
            "35_decoder_layers.0.norm1.LayerNorm_norm           [256]    [128, 29, 256]   \n",
            "36_decoder_layers.0.enc_dec_attn.Linear_wq    [256, 256]    [128, 29, 256]   \n",
            "37_decoder_layers.0.enc_dec_attn.Linear_wk    [256, 256]    [128, 26, 256]   \n",
            "38_decoder_layers.0.enc_dec_attn.Linear_wv    [256, 256]    [128, 26, 256]   \n",
            "39_decoder_layers.0.enc_dec_attn.Linear_wo    [256, 256]    [128, 29, 256]   \n",
            "40_decoder_layers.0.norm2.LayerNorm_norm           [256]    [128, 29, 256]   \n",
            "41_decoder_layers.0.ffn.Linear_linear1        [256, 512]    [128, 29, 512]   \n",
            "42_decoder_layers.0.ffn.Dropout_dropout                -    [128, 29, 512]   \n",
            "43_decoder_layers.0.ffn.Linear_linear2        [512, 256]    [128, 29, 256]   \n",
            "44_decoder_layers.0.norm3.LayerNorm_norm           [256]    [128, 29, 256]   \n",
            "45_decoder_layers.1.self_attn.Linear_wq       [256, 256]    [128, 29, 256]   \n",
            "46_decoder_layers.1.self_attn.Linear_wk       [256, 256]    [128, 29, 256]   \n",
            "47_decoder_layers.1.self_attn.Linear_wv       [256, 256]    [128, 29, 256]   \n",
            "48_decoder_layers.1.self_attn.Linear_wo       [256, 256]    [128, 29, 256]   \n",
            "49_decoder_layers.1.norm1.LayerNorm_norm           [256]    [128, 29, 256]   \n",
            "50_decoder_layers.1.enc_dec_attn.Linear_wq    [256, 256]    [128, 29, 256]   \n",
            "51_decoder_layers.1.enc_dec_attn.Linear_wk    [256, 256]    [128, 26, 256]   \n",
            "52_decoder_layers.1.enc_dec_attn.Linear_wv    [256, 256]    [128, 26, 256]   \n",
            "53_decoder_layers.1.enc_dec_attn.Linear_wo    [256, 256]    [128, 29, 256]   \n",
            "54_decoder_layers.1.norm2.LayerNorm_norm           [256]    [128, 29, 256]   \n",
            "55_decoder_layers.1.ffn.Linear_linear1        [256, 512]    [128, 29, 512]   \n",
            "56_decoder_layers.1.ffn.Dropout_dropout                -    [128, 29, 512]   \n",
            "57_decoder_layers.1.ffn.Linear_linear2        [512, 256]    [128, 29, 256]   \n",
            "58_decoder_layers.1.norm3.LayerNorm_norm           [256]    [128, 29, 256]   \n",
            "59_decoder_layers.2.self_attn.Linear_wq       [256, 256]    [128, 29, 256]   \n",
            "60_decoder_layers.2.self_attn.Linear_wk       [256, 256]    [128, 29, 256]   \n",
            "61_decoder_layers.2.self_attn.Linear_wv       [256, 256]    [128, 29, 256]   \n",
            "62_decoder_layers.2.self_attn.Linear_wo       [256, 256]    [128, 29, 256]   \n",
            "63_decoder_layers.2.norm1.LayerNorm_norm           [256]    [128, 29, 256]   \n",
            "64_decoder_layers.2.enc_dec_attn.Linear_wq    [256, 256]    [128, 29, 256]   \n",
            "65_decoder_layers.2.enc_dec_attn.Linear_wk    [256, 256]    [128, 26, 256]   \n",
            "66_decoder_layers.2.enc_dec_attn.Linear_wv    [256, 256]    [128, 26, 256]   \n",
            "67_decoder_layers.2.enc_dec_attn.Linear_wo    [256, 256]    [128, 29, 256]   \n",
            "68_decoder_layers.2.norm2.LayerNorm_norm           [256]    [128, 29, 256]   \n",
            "69_decoder_layers.2.ffn.Linear_linear1        [256, 512]    [128, 29, 512]   \n",
            "70_decoder_layers.2.ffn.Dropout_dropout                -    [128, 29, 512]   \n",
            "71_decoder_layers.2.ffn.Linear_linear2        [512, 256]    [128, 29, 256]   \n",
            "72_decoder_layers.2.norm3.LayerNorm_norm           [256]    [128, 29, 256]   \n",
            "73_fc                                       [256, 19214]  [128, 29, 19214]   \n",
            "\n",
            "                                               Params  Mult-Adds  \n",
            "Layer                                                             \n",
            "0_src_embedding                             2.774272M  2.774272M  \n",
            "1_pos_encoding.Dropout_dropout                      -          -  \n",
            "2_tgt_embedding                             4.918784M  4.918784M  \n",
            "3_pos_encoding.Dropout_dropout                      -          -  \n",
            "4_encoder_layers.0.self_attn.Linear_wq        65.792k    65.536k  \n",
            "5_encoder_layers.0.self_attn.Linear_wk        65.792k    65.536k  \n",
            "6_encoder_layers.0.self_attn.Linear_wv        65.792k    65.536k  \n",
            "7_encoder_layers.0.self_attn.Linear_wo        65.792k    65.536k  \n",
            "8_encoder_layers.0.norm1.LayerNorm_norm         512.0      256.0  \n",
            "9_encoder_layers.0.ffn.Linear_linear1        131.584k   131.072k  \n",
            "10_encoder_layers.0.ffn.Dropout_dropout             -          -  \n",
            "11_encoder_layers.0.ffn.Linear_linear2       131.328k   131.072k  \n",
            "12_encoder_layers.0.norm2.LayerNorm_norm        512.0      256.0  \n",
            "13_encoder_layers.1.self_attn.Linear_wq       65.792k    65.536k  \n",
            "14_encoder_layers.1.self_attn.Linear_wk       65.792k    65.536k  \n",
            "15_encoder_layers.1.self_attn.Linear_wv       65.792k    65.536k  \n",
            "16_encoder_layers.1.self_attn.Linear_wo       65.792k    65.536k  \n",
            "17_encoder_layers.1.norm1.LayerNorm_norm        512.0      256.0  \n",
            "18_encoder_layers.1.ffn.Linear_linear1       131.584k   131.072k  \n",
            "19_encoder_layers.1.ffn.Dropout_dropout             -          -  \n",
            "20_encoder_layers.1.ffn.Linear_linear2       131.328k   131.072k  \n",
            "21_encoder_layers.1.norm2.LayerNorm_norm        512.0      256.0  \n",
            "22_encoder_layers.2.self_attn.Linear_wq       65.792k    65.536k  \n",
            "23_encoder_layers.2.self_attn.Linear_wk       65.792k    65.536k  \n",
            "24_encoder_layers.2.self_attn.Linear_wv       65.792k    65.536k  \n",
            "25_encoder_layers.2.self_attn.Linear_wo       65.792k    65.536k  \n",
            "26_encoder_layers.2.norm1.LayerNorm_norm        512.0      256.0  \n",
            "27_encoder_layers.2.ffn.Linear_linear1       131.584k   131.072k  \n",
            "28_encoder_layers.2.ffn.Dropout_dropout             -          -  \n",
            "29_encoder_layers.2.ffn.Linear_linear2       131.328k   131.072k  \n",
            "30_encoder_layers.2.norm2.LayerNorm_norm        512.0      256.0  \n",
            "31_decoder_layers.0.self_attn.Linear_wq       65.792k    65.536k  \n",
            "32_decoder_layers.0.self_attn.Linear_wk       65.792k    65.536k  \n",
            "33_decoder_layers.0.self_attn.Linear_wv       65.792k    65.536k  \n",
            "34_decoder_layers.0.self_attn.Linear_wo       65.792k    65.536k  \n",
            "35_decoder_layers.0.norm1.LayerNorm_norm        512.0      256.0  \n",
            "36_decoder_layers.0.enc_dec_attn.Linear_wq    65.792k    65.536k  \n",
            "37_decoder_layers.0.enc_dec_attn.Linear_wk    65.792k    65.536k  \n",
            "38_decoder_layers.0.enc_dec_attn.Linear_wv    65.792k    65.536k  \n",
            "39_decoder_layers.0.enc_dec_attn.Linear_wo    65.792k    65.536k  \n",
            "40_decoder_layers.0.norm2.LayerNorm_norm        512.0      256.0  \n",
            "41_decoder_layers.0.ffn.Linear_linear1       131.584k   131.072k  \n",
            "42_decoder_layers.0.ffn.Dropout_dropout             -          -  \n",
            "43_decoder_layers.0.ffn.Linear_linear2       131.328k   131.072k  \n",
            "44_decoder_layers.0.norm3.LayerNorm_norm        512.0      256.0  \n",
            "45_decoder_layers.1.self_attn.Linear_wq       65.792k    65.536k  \n",
            "46_decoder_layers.1.self_attn.Linear_wk       65.792k    65.536k  \n",
            "47_decoder_layers.1.self_attn.Linear_wv       65.792k    65.536k  \n",
            "48_decoder_layers.1.self_attn.Linear_wo       65.792k    65.536k  \n",
            "49_decoder_layers.1.norm1.LayerNorm_norm        512.0      256.0  \n",
            "50_decoder_layers.1.enc_dec_attn.Linear_wq    65.792k    65.536k  \n",
            "51_decoder_layers.1.enc_dec_attn.Linear_wk    65.792k    65.536k  \n",
            "52_decoder_layers.1.enc_dec_attn.Linear_wv    65.792k    65.536k  \n",
            "53_decoder_layers.1.enc_dec_attn.Linear_wo    65.792k    65.536k  \n",
            "54_decoder_layers.1.norm2.LayerNorm_norm        512.0      256.0  \n",
            "55_decoder_layers.1.ffn.Linear_linear1       131.584k   131.072k  \n",
            "56_decoder_layers.1.ffn.Dropout_dropout             -          -  \n",
            "57_decoder_layers.1.ffn.Linear_linear2       131.328k   131.072k  \n",
            "58_decoder_layers.1.norm3.LayerNorm_norm        512.0      256.0  \n",
            "59_decoder_layers.2.self_attn.Linear_wq       65.792k    65.536k  \n",
            "60_decoder_layers.2.self_attn.Linear_wk       65.792k    65.536k  \n",
            "61_decoder_layers.2.self_attn.Linear_wv       65.792k    65.536k  \n",
            "62_decoder_layers.2.self_attn.Linear_wo       65.792k    65.536k  \n",
            "63_decoder_layers.2.norm1.LayerNorm_norm        512.0      256.0  \n",
            "64_decoder_layers.2.enc_dec_attn.Linear_wq    65.792k    65.536k  \n",
            "65_decoder_layers.2.enc_dec_attn.Linear_wk    65.792k    65.536k  \n",
            "66_decoder_layers.2.enc_dec_attn.Linear_wv    65.792k    65.536k  \n",
            "67_decoder_layers.2.enc_dec_attn.Linear_wo    65.792k    65.536k  \n",
            "68_decoder_layers.2.norm2.LayerNorm_norm        512.0      256.0  \n",
            "69_decoder_layers.2.ffn.Linear_linear1       131.584k   131.072k  \n",
            "70_decoder_layers.2.ffn.Dropout_dropout             -          -  \n",
            "71_decoder_layers.2.ffn.Linear_linear2       131.328k   131.072k  \n",
            "72_decoder_layers.2.norm3.LayerNorm_norm        512.0      256.0  \n",
            "73_fc                                       4.937998M  4.918784M  \n",
            "------------------------------------------------------------------------------------------------\n",
            "                          Totals\n",
            "Total params          16.584718M\n",
            "Trainable params      16.584718M\n",
            "Non-trainable params         0.0\n",
            "Mult-Adds              16.54784M\n",
            "================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wli169/miniconda3/envs/py38torch_arm/lib/python3.8/site-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_src_embedding</th>\n",
              "      <td>[256, 10837]</td>\n",
              "      <td>[128, 26, 256]</td>\n",
              "      <td>2774272.0</td>\n",
              "      <td>2774272.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_pos_encoding.Dropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 26, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_tgt_embedding</th>\n",
              "      <td>[256, 19214]</td>\n",
              "      <td>[128, 29, 256]</td>\n",
              "      <td>4918784.0</td>\n",
              "      <td>4918784.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_pos_encoding.Dropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 29, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_encoder_layers.0.self_attn.Linear_wq</th>\n",
              "      <td>[256, 256]</td>\n",
              "      <td>[128, 26, 256]</td>\n",
              "      <td>65792.0</td>\n",
              "      <td>65536.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69_decoder_layers.2.ffn.Linear_linear1</th>\n",
              "      <td>[256, 512]</td>\n",
              "      <td>[128, 29, 512]</td>\n",
              "      <td>131584.0</td>\n",
              "      <td>131072.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70_decoder_layers.2.ffn.Dropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 29, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71_decoder_layers.2.ffn.Linear_linear2</th>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>[128, 29, 256]</td>\n",
              "      <td>131328.0</td>\n",
              "      <td>131072.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72_decoder_layers.2.norm3.LayerNorm_norm</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[128, 29, 256]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73_fc</th>\n",
              "      <td>[256, 19214]</td>\n",
              "      <td>[128, 29, 19214]</td>\n",
              "      <td>4937998.0</td>\n",
              "      <td>4918784.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>74 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Kernel Shape      Output Shape  \\\n",
              "Layer                                                                      \n",
              "0_src_embedding                           [256, 10837]    [128, 26, 256]   \n",
              "1_pos_encoding.Dropout_dropout                       -    [128, 26, 256]   \n",
              "2_tgt_embedding                           [256, 19214]    [128, 29, 256]   \n",
              "3_pos_encoding.Dropout_dropout                       -    [128, 29, 256]   \n",
              "4_encoder_layers.0.self_attn.Linear_wq      [256, 256]    [128, 26, 256]   \n",
              "...                                                ...               ...   \n",
              "69_decoder_layers.2.ffn.Linear_linear1      [256, 512]    [128, 29, 512]   \n",
              "70_decoder_layers.2.ffn.Dropout_dropout              -    [128, 29, 512]   \n",
              "71_decoder_layers.2.ffn.Linear_linear2      [512, 256]    [128, 29, 256]   \n",
              "72_decoder_layers.2.norm3.LayerNorm_norm         [256]    [128, 29, 256]   \n",
              "73_fc                                     [256, 19214]  [128, 29, 19214]   \n",
              "\n",
              "                                             Params  Mult-Adds  \n",
              "Layer                                                           \n",
              "0_src_embedding                           2774272.0  2774272.0  \n",
              "1_pos_encoding.Dropout_dropout                  NaN        NaN  \n",
              "2_tgt_embedding                           4918784.0  4918784.0  \n",
              "3_pos_encoding.Dropout_dropout                  NaN        NaN  \n",
              "4_encoder_layers.0.self_attn.Linear_wq      65792.0    65536.0  \n",
              "...                                             ...        ...  \n",
              "69_decoder_layers.2.ffn.Linear_linear1     131584.0   131072.0  \n",
              "70_decoder_layers.2.ffn.Dropout_dropout         NaN        NaN  \n",
              "71_decoder_layers.2.ffn.Linear_linear2     131328.0   131072.0  \n",
              "72_decoder_layers.2.norm3.LayerNorm_norm      512.0      256.0  \n",
              "73_fc                                     4937998.0  4918784.0  \n",
              "\n",
              "[74 rows x 4 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model Summary\n",
        "from torchsummaryX import summary\n",
        "src, tgt = next(iter(train_dataloader))\n",
        "src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "\n",
        "tgt_input = tgt[:, :-1]\n",
        "tgt_output = tgt[:, 1:]\n",
        "src_mask = generate_src_mask(src, EN_VOCAB['<pad>'])\n",
        "tgt_mask = generate_tgt_mask(tgt_input, DE_VOCAB['<pad>'])\n",
        "\n",
        "summary(model, src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "# summary(model, src, tgt_input, src_mask, tgt_mask): \n",
        "# This line generates a summary of the model's architecture and \n",
        "# how the input data (src, tgt_input, and the masks) flows through it;\n",
        "# providing detailed insights about the model, \n",
        "# such as the size of each layer, the number of parameters, \n",
        "# the shape of the output at each stage, and the computational cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh1jjEftNb5z"
      },
      "source": [
        "## Train and Validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "b_f6RKBCKh8Z"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Note: The source sequences will be input in the encoder,\n",
        "    while the target sequences will be input in the decoder (as tgt_input sequences),\n",
        "    and also target output (as tgt_output sequences) in the loss computation.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    batch_bar = tqdm(\n",
        "        total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc=\"Train\"\n",
        "    )\n",
        "\n",
        "    for i, (src, tgt) in enumerate(dataloader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        # print(\"Src\", src.shape, \"Tgt\", tgt.shape)\n",
        "\n",
        "        # target as input (teacher forcing)\n",
        "        tgt_input = tgt[\n",
        "            :, :-1\n",
        "        ]  # (batch_size, max_tgt_seq_len - 1), so exlucing the last token <eos>\n",
        "        # target as output (for loss evaluation)\n",
        "        tgt_output = tgt[\n",
        "            :, 1:\n",
        "        ]  # (batch_size, max_tgt_seq_len - 1), so exlucing the first token <sos>\n",
        "\n",
        "        src_mask = generate_src_mask(src, EN_VOCAB[\"<pad>\"])\n",
        "        # (batch_size, 1, 1, max_seq_length)\n",
        "        tgt_mask = generate_tgt_mask(tgt_input, DE_VOCAB[\"<pad>\"])\n",
        "        # (batch_size, max_tgt_seq_len - 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if device.type == \"cpu\":\n",
        "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
        "            # output shape (batch_size, max_tgt_seq_len - 1, tgt_vocab_size)\n",
        "            # this is prediction at tgt_input (batch_size, max_tgt_seq_len - 1)\n",
        "            # i.e., prediction at the first token (after <sos>), and all tokens (including possibly <pad>), exlucing prediction for <eos>.\n",
        "            loss = criterion(output.reshape(-1, output.size(2)), tgt_output.reshape(-1))\n",
        "            # output.reshape(-1, output.size(2)): (batch_size * max_tgt_seq_len-1, output_vocab_size)\n",
        "            # tgt_output.reshape(-1): shape (batch_size * max_tgt_seq_length-1)\n",
        "\n",
        "            # CrossEntropyLoss function takes the reshaped output and\n",
        "            # tgt_output to compute the loss. It compares the predicted\n",
        "            # output (logits for each token in the vocabulary) with the actual\n",
        "            # target token indices.\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(src, tgt_input, src_mask, tgt_mask)\n",
        "                loss = criterion(\n",
        "                    output.reshape(-1, output.size(2)), tgt_output.reshape(-1)\n",
        "                )\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(total_loss / (i + 1)),\n",
        "            lr=\"{:.09f}\".format(float(optimizer.param_groups[0][\"lr\"])),\n",
        "        )\n",
        "        batch_bar.update()\n",
        "\n",
        "    # Return the average loss for the epoch\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    \"\"\"\n",
        "    Basically same procedure as the train_epoch, except no optimization.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_bleu_score = 0\n",
        "\n",
        "    batch_bar = tqdm(\n",
        "        total=len(dataloader),\n",
        "        dynamic_ncols=True,\n",
        "        leave=False,\n",
        "        position=0,\n",
        "        desc=\"Validate\",\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (src, tgt) in enumerate(dataloader):\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[\n",
        "                :, :-1\n",
        "            ]  # (batch_size, max_tgt_seq_len - 1), so exlucing <eos>\n",
        "            tgt_output = tgt[\n",
        "                :, 1:\n",
        "            ]  # (batch_size, max_tgt_seq_len - 1), so exlucing <sos>\n",
        "\n",
        "            src_mask = generate_src_mask(src, EN_VOCAB[\"<pad>\"])\n",
        "            tgt_mask = generate_tgt_mask(tgt_input, DE_VOCAB[\"<pad>\"])\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(src, tgt_input, src_mask, tgt_mask)\n",
        "                # output shape (batch_size, max_tgt_seq_len - 1, tgt_vocab_size)\n",
        "                # this is prediction at tgt_input (batch_size, max_tgt_seq_len - 1)\n",
        "                # i.e., prediction at the first token (after <sos>), and all tokens (including possibly <pad>), exlucing prediction for <eos>.\n",
        "                loss = criterion(\n",
        "                    output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1)\n",
        "                )\n",
        "\n",
        "            epoch_loss += loss.item()  # Accumulate the loss\n",
        "            # Calculate and accumulate the BLEU score\n",
        "            epoch_bleu_score += calculate_bleu(tgt_output, output.argmax(-1), DE_VOCAB)\n",
        "            # output.argmax(-1): shape (batch_size, max_tgt_seq_len - 1) token indices\n",
        "\n",
        "            batch_bar.set_postfix(\n",
        "                loss=\"{:.04f}\".format(epoch_loss / (i + 1)),\n",
        "                bleu=\"{:.04f}\".format(epoch_bleu_score / (i + 1)),\n",
        "            )\n",
        "\n",
        "            batch_bar.update()\n",
        "\n",
        "    # Normalize the loss and BLEU score by the number of minibatches in validation samples\n",
        "    epoch_loss /= len(dataloader)\n",
        "    epoch_bleu_score /= len(dataloader)\n",
        "\n",
        "    return epoch_loss, epoch_bleu_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK2eqqz9Vran"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B77gwz2R6SL",
        "outputId": "a607868d-6a0d-4f47-e629-ee9d0536df0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validate:   0%|          | 0/8 [00:00<?, ?it/s]                                      /Users/wli169/miniconda3/envs/py38torch_arm/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 5.8270 | Val Loss: 4.2420 | BLEU Score: 6.7642\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.8166 | Val Loss: 3.6280 | BLEU Score: 17.1142\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.3091 | Val Loss: 3.3153 | BLEU Score: 12.6427\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.9304 | Val Loss: 3.1390 | BLEU Score: 27.8041\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.6701 | Val Loss: 3.0689 | BLEU Score: 23.1824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "bleu_scores = [] \n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "\n",
        "    # Training\n",
        "    train_loss = train_epoch(model, train_dataloader, optimizer, criterion, DEVICE)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, bleu_score = validate_epoch(model, val_dataloader, criterion, DEVICE)\n",
        "    val_losses.append(val_loss)\n",
        "    bleu_scores.append(bleu_score)\n",
        "\n",
        "    # Update learning rate (commented out: since we update inside train_epoch)\n",
        "    # scheduler.step() \n",
        "\n",
        "    # Print results\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "    # Save the model with the best validation loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqf7bCNcrHW5"
      },
      "source": [
        "# Evaluate Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYWi2OV6WPzE",
        "outputId": "5feefabd-c1dd-402d-8013-674f4b944be3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 8/8 [00:35<00:00,  4.43s/it]\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test BLEU score: 29.65422317710202\n"
          ]
        }
      ],
      "source": [
        "test_out = evaluate_test_set_bleu(model, test_dataloader, EN_VOCAB, DE_VOCAB, DEVICE)\n",
        "print(\"Test BLEU score:\", test_out[0].score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Example Sentence and its Translation\n",
            "Source Sentence in English               : A man in a martial arts uniform in midair .\n",
            "Ground Truth Sentence in German          : Ein Mann in einem Karateanzug in der Luft .\n",
            "Machine Translated Sentence in German    : Ein Mann in einem Kampfsportler mitten in der Luft .\n"
          ]
        }
      ],
      "source": [
        "# Randomly select an example from the test set to display\n",
        "rand_index = random.randint(\n",
        "    0, len(test_dataset)\n",
        ")  # len(test_dataset): total number of test sentences\n",
        "print(\"\\n\\nExample Sentence and its Translation\")\n",
        "# Display the source sentence, ground truth, and machine-translated sentence\n",
        "# for the example, excluding <pad>, <sos> and <eos>\n",
        "print(\n",
        "    \"Source Sentence in English               :\",\n",
        "    \" \".join(\n",
        "        [\n",
        "            EN_VOCAB.itos[i]\n",
        "            for i in test_dataset[rand_index][0]\n",
        "            if EN_VOCAB.itos[i] not in [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "print(\"Ground Truth Sentence in German          :\", test_out[1][rand_index])\n",
        "print(\"Machine Translated Sentence in German    :\", test_out[2][rand_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['Ein Mann mit einem orangefarbenen Hut , der etwas <unk> .',\n",
              "  'Ein Boston Terrier läuft über <unk> Gras vor einem weißen Zaun .',\n",
              "  'Ein Mädchen in einem Karateanzug bricht ein Brett mit einem Tritt .',\n",
              "  'Fünf Leute in Winterjacken und mit Helmen stehen im Schnee mit <unk> im Hintergrund .',\n",
              "  'Leute Reparieren das Dach eines Hauses .'],\n",
              " ['Ein Mann mit einem orangen Hut starrt auf etwas .',\n",
              "  'Ein Schäferhund rennt auf einer weißen Wiese vor einem weißen Zaun .',\n",
              "  'Ein Mädchen in Karateanzügen Kleidung hat einen Stock vor einem Tritt .',\n",
              "  'Fünf Personen in Jacken und mit Helmen stehen im Schnee , im Hintergrund .',\n",
              "  'Leute reparieren das Dach eines Hauses .'])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check Ground Truth Sentence in German, and Machine Translated Sentence in German\n",
        "test_out[1][0:5], test_out[2][0:5]\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lSHCF8Roet4a",
        "_jXgt0QI7J7f"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
