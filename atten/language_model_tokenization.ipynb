{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Language Model (Tokenization and BLEU)\n",
        "\n",
        "Wei Li"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.12.1\n",
            "2.3.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import sacrebleu\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from typing import Tuple\n",
        "import torchdata\n",
        "import spacy\n",
        "print(torch.__version__)\n",
        "print(sacrebleu.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import Multi30k\n",
        "from collections import Counter\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', '.']\n",
            "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', '.']\n"
          ]
        }
      ],
      "source": [
        "# Initialize tokenizers for English and German using spaCy models\n",
        "# 'spacy' indicates the type of tokenizer (from spaCy)\n",
        "# 'en_core_web_sm' is the small English model, 'de_core_news_sm' is the small German model\n",
        "en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "#  it returns a spaCy tokenizer instance.\n",
        "de_tokenizer = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
        "\n",
        "# Define a function to tokenize English text\n",
        "# Uses the English tokenizer to split the text into tokens\n",
        "def tokenize_en(text):\n",
        "    # convert text to string\n",
        "    doc = en_tokenizer(str(text))\n",
        "    return [token for token in doc]\n",
        "\n",
        "sample=\"Hello, world! This is a test sentence.\"\n",
        "print(en_tokenizer(sample))\n",
        "print(tokenize_en(sample))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a class to create a vocabulary from text\n",
        "class VOCAB:\n",
        "    # Initializer for the VOCAB class\n",
        "    # tokenizer: a function for tokenizing text\n",
        "    # min_freq: minimum frequency for a word to be included in the vocabulary\n",
        "    # data: the text data to build the vocabulary from\n",
        "    # special_tokens: a list of special tokens (like <pad>, <sos>, etc.)\n",
        "    def __init__(self, tokenizer, min_freq=2, data = None, special_tokens=['<pad>', '<sos>', '<eos>', '<unk>']):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.min_freq = min_freq\n",
        "        self.special_tokens = special_tokens\n",
        "        self.build_vocab(data)\n",
        "\n",
        "    # Method to build the vocabulary\n",
        "    def build_vocab(self, data):\n",
        "        counter = Counter()\n",
        "        # Iterate over the data, tokenize each text and update the counter\n",
        "        for text in tqdm(data):\n",
        "            tokens = self.tokenizer(text)\n",
        "            counter.update(tokens)\n",
        "\n",
        "        # Filter tokens that meet the minimum frequency threshold\n",
        "        voc_ls = [token for token, freq in counter.items() if freq >= self.min_freq]\n",
        "\n",
        "        # Add special tokens to the start of the tokens list\n",
        "        voc_ls = self.special_tokens + voc_ls\n",
        "\n",
        "        # Create string-to-index mapping\n",
        "        self.stoi = {token: index for index, token in enumerate(voc_ls)}\n",
        "        self.itos = voc_ls  # Also create index-to-string mapping\n",
        "\n",
        "    # Return the length of the vocabulary\n",
        "    def __len__(self):\n",
        "        return len(self.stoi)\n",
        "\n",
        "    # Retrieve an item index from the vocabulary; return index for '<unk>' for unknown tokens\n",
        "    def __getitem__(self, token):\n",
        "        return self.stoi.get(token, self.stoi['<unk>'])\n",
        "        # This line attempts to retrieve the index corresponding to the token from the stoi dictionary. \n",
        "        # If the token is not found in the dictionary, it returns the index of a special token <unk> instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### counter\n",
        "\n",
        "When `counter.update(tokens)` is called, it iterates over each token in the tokens list.\n",
        "For each token, if it's not already in the Counter, it's added with a count of 1. If it's already present, its count is incremented by 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 2949.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hello', 'world']\n",
            "['hello', 'python', 'world']\n",
            "['hello', 'python', '!']\n",
            "Tokens above frequency threshold: ['hello', 'world', 'python']\n",
            "Full token frequency counter: Counter({'hello': 3, 'world': 2, 'python': 2, '!': 1})\n",
            "\n",
            "{'hello': 0, 'world': 1, 'python': 2}\n",
            "['hello', 'world', 'python']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample data: a list of sentences\n",
        "data = [\"hello world\", \"hello python world\", \"hello python!\"]\n",
        "\n",
        "# Create a Counter object for counting token frequencies\n",
        "counter = Counter()\n",
        "\n",
        "# Define the minimum frequency threshold for tokens\n",
        "min_freq = 2\n",
        "\n",
        "# Iterate over each text in the data, tokenize it, and update the counter\n",
        "for text in tqdm(data):\n",
        "    tokens = en_tokenizer(text)  # Tokenize the current text\n",
        "    print(tokens)\n",
        "    counter.update(tokens)  # Update the counter with the tokens\n",
        "\n",
        "# Filter out tokens that meet or exceed the minimum frequency threshold\n",
        "voc_ls = [token for token, freq in counter.items() if freq >= min_freq]\n",
        "# for token, freq in counter.items(): This iterates over each key-value pair in the Counter object.\n",
        "\n",
        "# Print the filtered tokens and the full counter for comparison\n",
        "print(\"Tokens above frequency threshold:\", voc_ls)\n",
        "print(\"Full token frequency counter:\", counter)\n",
        "print()\n",
        "# Create string-to-index mapping\n",
        "stoi = {token: index for index, token in enumerate(voc_ls)}\n",
        "print(stoi)\n",
        "# index-to-string mapping\n",
        "itos= voc_ls \n",
        "print(itos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Two young, White males are outside near many bushes.', 'Several men in hard hats are operating a giant pulley system.', 'A little girl climbing into a wooden playhouse.'] 29001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29001/29001 [00:00<00:00, 46783.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Vocab Size English 10837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# File paths for the English and German training data\n",
        "en_file = \"../data/Multi30k/train/train.en\"\n",
        "\n",
        "# Open and read the English training data file\n",
        "with open(en_file, \"r\", encoding=\"utf8\") as f:\n",
        "    train_data_en = [text.strip() for text in f.readlines()]\n",
        "\n",
        "# train_data_en is a list of strings\n",
        "# print the first three strings, and the size of the data\n",
        "print(train_data_en[0:3], len(train_data_en))\n",
        "\n",
        "\n",
        "# Create vocabulary objects for English and German training data\n",
        "# Here, min_freq is set to 1, meaning all tokens are included\n",
        "EN_VOCAB = VOCAB(tokenize_en, min_freq=1, data = train_data_en)\n",
        "\n",
        "# Print the sizes of the created English and German vocabularies\n",
        "print(\"\\nVocab Size English\", len(EN_VOCAB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<pad>', '<sos>', '<eos>', 'Hallo', 'Welt', 'wie', 'geht', 'es', 'dir', '?']\n",
            "Hallo Welt wie geht es dir ?\n"
          ]
        }
      ],
      "source": [
        "# example of converting padded sequence (with special tokens) to original sequence of tokens\n",
        "\n",
        "# Define a simple vocabulary mapping words to token indices\n",
        "DE_VOCAB = {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'Hallo': 3, 'Welt': 4, 'wie': 5, 'geht': 6, 'es': 7, 'dir': 8, '?': 9}\n",
        "\n",
        "# Create a list for index-to-string (itos) mapping\n",
        "itos = [word for word, index in sorted(DE_VOCAB.items(), key=lambda x: x[1])]\n",
        "\n",
        "# Example sequence of token indices (tgt)\n",
        "tgt = [1, 3, 4, 5, 6, 7, 8, 9, 2, 0, 0]  # Corresponds to: \"<sos> Hallo Welt wie geht es dir ? <eos> <pad> <pad>\"\n",
        "\n",
        "# Convert the sequence of token indices to a string, excluding special tokens\n",
        "ref = ' '.join([itos[t] for t in tgt if t not in (DE_VOCAB['<pad>'], DE_VOCAB['<eos>'], DE_VOCAB['<sos>'])])\n",
        "\n",
        "print(itos) # list of tokens\n",
        "print(ref)  # Output the converted sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BLEU (Billingual Evaluation Understudy) Score\n",
        "\n",
        "Note that there are two modules that compute BLEU score, one is `sacrebleu` and one is from\n",
        "`nltk.translate.bleu_score`. We use `sacrebleu` here.\n",
        "\n",
        "https://www.nltk.org/api/nltk.translate.gleu_score.html\n",
        "\n",
        "https://github.com/mjpost/sacrebleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BLEU Score Explanation with Example\n",
        "\n",
        "BLEU (Bilingual Evaluation Understudy) score is a metric for evaluating a machine-translated text against a set of high-quality reference translations. Let's understand this using a simple example.\n",
        "\n",
        "Example Setup\n",
        "- **Reference Sentence**: \"The quick brown fox jumps over the lazy dog.\"\n",
        "- **Machine-Translated Sentence**: \"The fast brown fox jumped over the lazy dog.\"\n",
        "\n",
        "N-gram Precision: we compare n-grams (continuous sequences of n items from the text) between the machine-translated sentence and the reference sentence.\n",
        "\n",
        "1-gram (Unigram) Precision\n",
        "- Reference: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "- Translation: [\"The\", \"fast\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "- Matches: 7 out of 9 (\"The\", \"brown\", \"fox\", \"over\", \"the\", \"lazy\", \"dog\")\n",
        "- **Precision**: 7/9\n",
        "\n",
        "2-gram (Bigram) Precision\n",
        "- Reference bigrams: [\"The quick\", \"quick brown\", ... , \"lazy dog\"]\n",
        "- Translation bigrams: [\"The fast\", \"fast brown\", ... , \"lazy dog\"]\n",
        "- Matches: 4 out of 8 (\"brown fox\", \"over the\", \"the lazy\", \"lazy dog\")\n",
        "- **Precision**: 4/8\n",
        "\n",
        "(Similar calculations for 3-gram and 4-gram.)\n",
        "\n",
        "Brevity Penalty (BP): BP penalizes translations that are too short.\n",
        "\n",
        "- **Hypothesis Length (h)**: 9 (Length of the machine-translated sentence)\n",
        "- **Reference Length (r)**: 9 (Length of the reference sentence)\n",
        "- **BP Calculation**:\n",
        "  - If h ≤ r, BP = exp(1 - r/h)\n",
        "  - If h > r, BP = 1\n",
        "- In our example, h = r, so BP = 1 (no penalty).\n",
        "\n",
        "Calculate BLEU Score:\n",
        "$$\n",
        "\\text{BLEU} = \\text{BP} \\cdot \\big(\\prod_{n=1}^N p_n\\big)^{1/N}= \\text{BP} \\cdot \\exp\\left(  \\frac{1}{N}\\sum_{n=1}^{N} \\log p_n \\right)\n",
        "$$\n",
        "\n",
        "$p_n$ is the precision for n-grams.; $\\sum_{n=1}^{N}$ represents the sum over all n-grams (from 1-gram to N-gram), $\\frac{1}{N}$ is the weight given to each n-gram (which is equal for all n-grams in the standard BLEU score calculation).\n",
        "\n",
        "The geometric mean $\\big(\\prod_{n=1}^N p_n\\big)^{1/N}$ ensures that a translation needs to perform well across all these different n-gram sizes to achieve a high score. The geometric mean is particularly sensitive to low values in a way that the arithmetic mean is not. If a translation scores very poorly in any n-gram category, the geometric mean will significantly lower the overall score. \n",
        "\n",
        "The BLEU score is a value between 0 and 100, where higher scores indicate better translations. It considers precision for up to 4-grams and adjusts for translation length. However, it does not account for semantic accuracy or grammatical correctness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3.1\n",
            "BLEU Score: 46.713797772819994\n",
            "Individual n-gram scores: [80.0, 55.55555555555556, 37.5, 28.571428571428573]\n",
            "Brevity Penalty: 1.0\n",
            "Translation length ratio: 1.0\n"
          ]
        }
      ],
      "source": [
        "import sacrebleu\n",
        "print(sacrebleu.__version__)\n",
        "# sacrebleu 2.3.1\n",
        "\n",
        "# Define the reference and the machine-translated sentences\n",
        "reference = [\"The quick brown fox jumps over the lazy dog.\"]\n",
        "translation = \"The fast brown fox jumped over the lazy dog.\"\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu = sacrebleu.corpus_bleu([translation], [reference])\n",
        "\n",
        "# Print the BLEU score and additional details\n",
        "print(f\"BLEU Score: {bleu.score}\")\n",
        "print(f\"Individual n-gram scores: {bleu.precisions}\")\n",
        "print(f\"Brevity Penalty: {bleu.bp}\")\n",
        "print(f\"Translation length ratio: {bleu.sys_len / bleu.ref_len}\")\n",
        "\n",
        "# The sacrebleu tool is designed to apply some more sophisticated counting rules, so \n",
        "# the nuanced handling of n-grams leads to the differences in precision scores obtained here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BLEU = 100.00 100.0/100.0/100.0/100.0 (BP = 1.000 ratio = 1.000 hyp_len = 17 ref_len = 17)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can also use more than one reference set\n",
        "\n",
        "refs = [ # First set of references\n",
        "             ['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.'],\n",
        "            # Second set of references\n",
        "             ['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.'],\n",
        "       ]\n",
        "sys = ['The dog bit the man.', \"No one was surprised.\", 'The man bit him first.']\n",
        "\n",
        "sacrebleu.corpus_bleu(sys, refs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BLEU = 100.00 100.0/100.0/100.0/100.0 (BP = 1.000 ratio = 1.000 hyp_len = 18 ref_len = 18)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "refs = [ # First set of references\n",
        "             ['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.'],\n",
        "            # Second set of references\n",
        "             ['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.'],\n",
        "       ]\n",
        "sys = ['The dog bit the man.', \"It was not unexpected.\", 'The man had bitten the dog.']\n",
        "\n",
        "sacrebleu.corpus_bleu(sys, refs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. References (refs):\n",
        "\n",
        "refs is a list of lists, where each inner list contains different reference translations for the same set of sentences.\n",
        "There are two sets of reference translations provided for each sentence in this example.\n",
        "For instance, the first sentence has two reference translations: \"The dog bit the man.\" and \"The dog had bit the man.\" If the transalted sentence matches either one of the two exactly, then it is a perfect match.\n",
        "\n",
        "2. System Output (sys):\n",
        "\n",
        "sys is a list containing the hypothesis sentences (or system translations).\n",
        "These are the sentences produced by the translation model that you want to evaluate.\n",
        "In this case, there are three sentences in sys.\n",
        "\n",
        "3. BLEU Score Calculation:\n",
        "\n",
        "`sacrebleu.corpus_bleu(sys, refs)` calculates the BLEU score by comparing each sentence in sys against the corresponding set of reference sentences in refs.\n",
        "The BLEU score is calculated over the entire corpus (all sentences) rather than individual sentences.\n",
        "The BLEU score includes several components:\n",
        "\n",
        "- BLEU = 48.53: This is the overall BLEU score, a weighted average of the n-gram precisions, adjusted by the Brevity Penalty.\n",
        "- 82.4/50.0/45.5/37.5: These are the individual n-gram precision scores for 1-gram, 2-gram, 3-gram, and 4-gram matches, respectively.\n",
        "- BP = 0.943: The Brevity Penalty value. A BP of 1 means no penalty, and a value less than 1 penalizes shorter translations.\n",
        "- ratio = 0.944: The ratio of the hypothesis length to the reference length.\n",
        "- hyp_len = 17: The total length of the hypothesis (system output).\n",
        "- ref_len = 18: The total length of the reference translations.\n",
        "\n",
        "The BLEU score and its components offer insights into various aspects of translation accuracy and fluency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BLEU = 36.21 35.3/35.7/36.4/37.5 (BP = 1.000 ratio = 2.833 hyp_len = 17 ref_len = 6)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# by replacing using either None or the empty string '', we can remove the reference sentence\n",
        "\n",
        "refs = [ # First set of references\n",
        "             ['The dog bit the man...ouch','',''],\n",
        "            # Second set of references\n",
        "             ['The dog bit the man.','','']\n",
        "       ]\n",
        "sys = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
        "\n",
        "sacrebleu.corpus_bleu(sys, refs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BLEU = 36.21 35.3/35.7/36.4/37.5 (BP = 1.000 ratio = 2.833 hyp_len = 17 ref_len = 6)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# by replacing using either None or the empty string '', we can remove the reference sentence\n",
        "\n",
        "refs = [ # First set of references\n",
        "             [\"It wasn't surprising.\",'',''],\n",
        "            # Second set of references\n",
        "             ['The dog bit the man.','','']\n",
        "       ]\n",
        "sys = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
        "\n",
        "sacrebleu.corpus_bleu(sys, refs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BLEU = 100.00 100.0/100.0/100.0/100.0 (BP = 1.000 ratio = 1.000 hyp_len = 6 ref_len = 6)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# what happens if we omit the reference entirely by even dropping ''. \n",
        "# it seems then, it would mean do not make comparison for those sentences at all.\n",
        "\n",
        "refs = [ # First set of references\n",
        "             [\"It wasn't surprising.\"],\n",
        "            # Second set of references\n",
        "             ['The dog bit the man.']\n",
        "       ]\n",
        "sys = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
        "\n",
        "sacrebleu.corpus_bleu(sys, refs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### playground: explain sacrebleu.corpus_bleu in translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['Der Hund läuft.'],\n",
              " ['Das Wetter ist schön.'],\n",
              " ['Das Auto ist neu.'],\n",
              " ['Das Haus ist groß.']]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example to illustrate ground_truth_sentences.extend([[tgt] for tgt in tgt_sentences])\n",
        "\n",
        "# Let's assume we have two batches of ground truth sentences (tgt_sentences) from a test dataset\n",
        "batch1_tgt_sentences = [\"Der Hund läuft.\", \"Das Wetter ist schön.\"]\n",
        "batch2_tgt_sentences = [\"Das Auto ist neu.\", \"Das Haus ist groß.\"]\n",
        "\n",
        "# Initialize an empty list for ground_truth_sentences\n",
        "ground_truth_sentences = []\n",
        "\n",
        "# Extending ground_truth_sentences with the first batch\n",
        "ground_truth_sentences.extend([[tgt] for tgt in batch1_tgt_sentences])\n",
        "# Extending ground_truth_sentences with the second batch\n",
        "ground_truth_sentences.extend([[tgt] for tgt in batch2_tgt_sentences])\n",
        "\n",
        "# Print the ground_truth_sentences list\n",
        "ground_truth_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score: 100.00000000000004\n"
          ]
        }
      ],
      "source": [
        "# Translated sentences\n",
        "translated_sentences = [\n",
        "    \"Der Hund rennt.\",\n",
        "    \"Das Wetter ist gut.\",\n",
        "    \"Ein neues Auto.\",\n",
        "    \"Das Haus ist großartig.\",\n",
        "]\n",
        "\n",
        "# Ground truth sentences\n",
        "ground_truth_sentences = [\n",
        "    [\n",
        "        \"Der Hund rennt.\",\n",
        "        \"Das Wetter ist gut.\",\n",
        "        \"Ein neues Auto.\",\n",
        "        \"Das Haus ist großartig.\",\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Calculate the BLEU score using sacrebleu\n",
        "bleu_score = sacrebleu.corpus_bleu(translated_sentences, ground_truth_sentences).score\n",
        "\n",
        "print(f\"BLEU score: {bleu_score}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lSHCF8Roet4a",
        "_jXgt0QI7J7f"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
