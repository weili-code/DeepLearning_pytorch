{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"pFvgJbAu50m8"},"source":["# Language Model Basics\n","\n","Wei Li"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":564,"status":"ok","timestamp":1666533005617,"user":{"displayName":"Soumya Empran","userId":"05313861740772333512"},"user_tz":240},"id":"mcIAFm9g50m9","outputId":"3a72c4cb-aae6-4078-c143-d2cd1545851a"},"outputs":[{"data":{"text/plain":["'cpu'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.utils.rnn as rnn\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","import numpy as np\n","import time\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","DEVICE"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Embeddings\n","\n","Embeddings are a way to convert discrete categorical variables (e.g., words represented as indices) into continuous vectors. The purpose of this is to capture more meaningful representations of the words.\n","\n","In the context of natural language processing, embeddings are used to capture semantic meanings of words. Words that are semantically similar are mapped to vectors that are close to each other in the embedding space. This is much more informative than using one-hot encoded vectors, where each word is represented as a vector of 0s with a single 1, and all vectors are at the same distance from each other.\n","\n","Let's consider an example. We have a vocabulary of four words: {'hello': 0, 'world': 1, 'good': 2, 'day': 3}. If we use one-hot encoding, 'hello' might be represented as [1, 0, 0, 0], 'world' as [0, 1, 0, 0], and so on. In this representation, all words are equidistant from each other, which doesn't capture any semantic relationships between the words.\n","\n","On the other hand, with embeddings, each word is represented as a dense vector of continuous values. For example, 'hello' might be represented as [0.1, 0.3, -0.2, 0.8, 0.5] and 'world' as [0.2, 0.3, -0.15, 0.85, 0.45]. Notice that these vectors are not just random. They are learned during the training of the model. The model adjusts these vectors to reduce the prediction error on the training data.\n","\n","So, if 'hello' and 'world' often appear in similar contexts in the training data, their embeddings will be adjusted to be close to each other. This way, the model learns to capture the semantic relationships between words. For example, synonyms would be close to each other in the embedding space because they often appear in similar contexts.\n","\n","**The use of embedding**: When we first initialize an embedding layer in a model (like an LSTM or a Transformer), the embedding table is filled with random numbers. Each word in our vocabulary is assigned one of these random vectors.\n","\n","During training, the model receives input data (e.g., the words in our text, represented as indices), looks up the corresponding vectors in the embedding table, and uses those vectors to make predictions. The model's predictions are compared to the actual target values, and the difference (the error) is used to update the model's parameters, including the vectors in the embedding table. This is done through a process called backpropagation and an optimization algorithm like stochastic gradient descent (SGD).\n","\n","The goal is to adjust the vectors in such a way that the model's predictions improve. This often means that words that are used in similar contexts or have similar meanings end up with similar vectors. The exact values and similarities will depend on the data and the specifics of the training process.\n","\n","So while the vectors are retrieved from the embedding table in a deterministic way during any single pass (forward or backward) through the model, the contents of that table change over the course of training. The vectors are not just random, they are learned based on the data."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Pytorch module**:\n","\n","https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n","\n","`nn.Embedding` is a simple lookup table that stores embeddings of a fixed dictionary and size.\n","This module is often used to store word embeddings and retrieve them using **indices**. \n","\n","- Input: $(*)$, IntTensor or LongTensor of arbitrary shape containing the indices to extract\n","- Output: $(*, H)$, where * is the input shape and $H=$ embedding_dim\n","\n","When you create an embedding layer in PyTorch with `nn.Embedding(vocab_size, embed_size)`, it initializes a table of size vocab_size x embed_size with random values. Each row in this table corresponds to the dense vector representation of a word (or whatever your indices represent). The vocab_size is the number of unique words (or indices) you have, and embed_size is the dimensionality of the output vectors you want."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Numerical examples:\n","\n","Let's consider a simple example of a text classification task. Suppose we have a vocabulary of four words: {'hello': 0, 'world': 1, 'good': 2, 'day': 3}. Each word is mapped to a unique index.\n","\n","We use indices instead of the words themselves because computers understand numbers better than text. By representing words as indices, we can easily convert them into dense vectors of fixed size (word embeddings), which can be processed efficiently by machine learning models.\n","\n","Now, let's say we have a sentence \"hello world\". We first convert this sentence into indices: [0, 1].\n","\n","We want to feed this sentence into a model to predict some target value (for example, the sentiment of the sentence). But before we can do that, we need to convert these indices into word embeddings."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.4084, -0.0216,  2.0937, -0.5386,  0.4335],\n","        [-0.4920,  0.2761,  0.9611, -0.5332, -0.1987]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}],"source":["import torch\n","from torch import nn\n","\n","# Create an embedding layer\n","vocab_size = 4  # We have 4 words in our vocabulary\n","embed_size = 5  # We want to represent each word as a 5-dimensional vector\n","embedding = nn.Embedding(vocab_size, embed_size)\n","\n","# Convert our sentence \"hello world\" into indices\n","sentence = torch.tensor([0, 1])  # \"hello\" is 0 and \"world\" is 1\n","\n","# Convert the indices into word embeddings\n","embeddings = embedding(sentence)\n","\n","print(embeddings)\n","# The output will be a 2D tensor of shape (2, 5). \n","# Each row is a 5-dimensional vector representing a word in the sentence.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Another example: let's say we have a vocabulary of 5 unique characters, represented as integers: 0, 1, 2, 3, 4. And we want to represent each character as a 3-dimensional embedding. So we create an embedding layer like this:"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["vocab_size = 5\n","embed_size = 3\n","embedding = nn.Embedding(vocab_size, embed_size)\n","# This creates an embedding layer that can transform any integer from 0 to 4 into a 3-dimensional vector."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now let's say we have a batch of 2 sequences (sequences of indices), each of length 4, represented as a 2D tensor:"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([2, 4])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["seq_batch = torch.tensor([[0, 1, 2, 3], [2, 3, 4, 0]])\n","seq_batch.shape #  2 sequences, each of length 4."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The embedding layer transforms each integer in the tensor into its corresponding 3-dimensional embedding. So the output embed is a 3D tensor with shape (2, 4, 3): 2 sequences, each of length 4, and each integer represented as a 3-dimensional vector."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([2, 4, 3])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["embed = embedding(seq_batch)\n","embed.shape"]},{"cell_type":"markdown","metadata":{},"source":["## pad and pack\n","\n","It is important to pack a paded sequence before passing it through an RNN as well as pad\n","a packed sequence after computation from the RNN.\n","\n","**a list of sequences (sorted by length descending) -> input pad_sequence -> `rnn.pack_padded_sequence()`->PackedSequence object -> LSTM -> PackedSequence object -> `rnn.pad_packed_sequence()`-> output pad_sequence**\n","\n","e.g., \n","input pad_sequence (B, T, feature size)  -> nn.LSTM (feature size, hidden_size) -> output pad_sequence (B, T, hidden_size)"]},{"cell_type":"markdown","metadata":{},"source":["### LSTM with input sequences of variable lengths\n","\n","In PyTorch, the LSTM model `torch.nn.LSTM()` can process sequences of variable lengths using `PackedSequence` objects. `PackedSequence` is a class in PyTorch that holds the data and list of `batch_sizes` of a packed sequence.\n","\n","All RNN modules, including LSTM, accept packed sequences as inputs.\n","To process variable-length sequences with LSTM, you can follow these steps:\n","\n","1. Sort the sequences by length: Before packing the sequences, sort them in descending order based on their lengths. This is required for the efficient processing of variable-length sequences.\n","2. Pad the sequences: Pad the sequences with zeros (or any other appropriate value) to make them all have the same length. This can be done using the `torch.nn.utils.rnn.pad_sequence()` function.\n","3. Pack the padded sequences: Use the `torch.nn.utils.rnn.pack_padded_sequence()` function to pack the padded sequences into a `PackedSequence` object . This function takes the input tensor and the lengths of the sequences as arguments.\n","4. Process the sequences with LSTM: Pass the `PackedSequence` object to the LSTM model. The LSTM model will process the sequences efficiently, taking into account their actual lengths .\n","5. Unpack the output: After processing the sequences with LSTM, you can use the `torch.nn.utils.rnn.pad_packed_sequence()` function to unpack the output back into padded sequences. This will give you the output for each time step in the original sequences (with padding)."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 2]) torch.Size([2, 2]) torch.Size([1, 2])\n"]}],"source":["import torch\n","from torch import nn\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","\n","# Create some example sequences, each shape (time_len, features_dim)\n","seq1 = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])  # (3, 2)\n","seq2 = torch.tensor([[7.0, 8.0], [9.0, 10.0]])  # (2, 2)\n","seq3 = torch.tensor([[11.0, 12.0]])  # (1, 2)\n","\n","print(seq1.shape, seq2.shape, seq3.shape)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[ 1.,  2.],\n","         [ 3.,  4.],\n","         [ 5.,  6.]],\n","\n","        [[ 7.,  8.],\n","         [ 9., 10.],\n","         [ 0.,  0.]],\n","\n","        [[11., 12.],\n","         [ 0.,  0.],\n","         [ 0.,  0.]]]) <class 'torch.Tensor'> torch.Size([3, 3, 2])\n"]}],"source":["# Pad the sequences\n","padded_sequences = pad_sequence([seq1, seq2, seq3], batch_first=True)\n","\n","print(padded_sequences, type(padded_sequences), padded_sequences.shape)\n","# padded_sequences.shape: torch.Size([3, 3, 2]= (B, T, F).\n","# so padded_sequences are grouped in 3 sequences, and each sequence has array 3 by 2."]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"./images/pack_padded_seq.png\" width=\"600\" height=\"430\">\n","\n","Source: https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([6, 2])\n","\n"]},{"data":{"text/plain":["PackedSequence(data=tensor([[ 1.,  2.],\n","        [ 7.,  8.],\n","        [11., 12.],\n","        [ 3.,  4.],\n","        [ 9., 10.],\n","        [ 5.,  6.]]), batch_sizes=tensor([3, 2, 1]), sorted_indices=tensor([0, 1, 2]), unsorted_indices=tensor([0, 1, 2]))"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# list of sequence lengths of each batch element\n","# first batch (sequence) has sequence (time) lengths 3,\n","# second batch has sequence (time) lengths 2,\n","# third batch has sequence (time) lengths 1,\n","seq_lengths = torch.tensor([3, 2, 1])\n","\n","# Pack the padded sequences\n","packed_sequences = pack_padded_sequence(\n","    padded_sequences, seq_lengths, batch_first=True, enforce_sorted=False\n",")\n","\n","type(packed_sequences)  # torch.nn.utils.rnn.PackedSequence\n","print(packed_sequences.data.shape)\n","print()\n","packed_sequences"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([6, 4])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Create an LSTM model\n","lstm = nn.LSTM(input_size=2, hidden_size=4, batch_first=True)\n","\n","# Process the packed sequences with LSTM\n","output, (hn, cn) = lstm(packed_sequences)\n","# output is also a PackedSequence object, the return of packed_sequences through LSTM.\n","# output.data has shape (6, H=4)\n","\n","output.data.shape"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 3, 4]) tensor([[[-3.4508e-02, -1.7152e-02, -6.1093e-02,  1.2922e-02],\n","         [-3.8915e-02, -1.6568e-02,  5.7424e-02,  6.1641e-03],\n","         [-1.4188e-02, -1.1229e-02,  2.7400e-01,  1.7242e-03]],\n","\n","        [[-1.2410e-03, -1.2493e-04,  3.1285e-01,  1.1075e-04],\n","         [-4.5575e-04, -8.3041e-05,  4.9538e-01,  3.6702e-05],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n","\n","        [[-3.5044e-05, -1.6551e-06,  3.8911e-01,  1.7866e-06],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n","       grad_fn=<IndexSelectBackward0>)\n","\n","compare to padded_sequences (input)\n","torch.Size([3, 3, 2]) tensor([[[ 1.,  2.],\n","         [ 3.,  4.],\n","         [ 5.,  6.]],\n","\n","        [[ 7.,  8.],\n","         [ 9., 10.],\n","         [ 0.,  0.]],\n","\n","        [[11., 12.],\n","         [ 0.,  0.],\n","         [ 0.,  0.]]])\n"]}],"source":["# This line unpacks the output from the LSTM layer back into padded sequences.\n","# Unpack the output\n","unpacked_output, _ = pad_packed_sequence(output, batch_first=True)\n","\n","print(unpacked_output.shape, unpacked_output)\n","# shape (3, 3, 4) : (B, T, hidden_size)\n","print()\n","print(\"compare to padded_sequences (input)\")\n","\n","print(padded_sequences.shape, padded_sequences)\n","# shape (3, 3, 2) : (B, T, feature size)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.17"}},"nbformat":4,"nbformat_minor":0}
